# Experiment Design: Structure-Factorized Attention (SFA) for Document-Centric MLLMs

**Experiment ID**: EXP-20260220-001
**Date**: 2026-02-20
**Author**: Juyeon
**Status**: In Progress (Baseline ì™„ë£Œ, SFA Fine-tuning ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ, ì‹¤í–‰ ëŒ€ê¸°)
**Last Updated**: 2026-02-20 (v2 â€” cross-architecture ìš”êµ¬ì‚¬í•­ ë°˜ì˜)

> ì´ ë¬¸ì„œê°€ **ìœ ì¼í•œ ì‹¤í—˜ ë§ˆìŠ¤í„° ë¬¸ì„œ**ì…ë‹ˆë‹¤.
> ì‹¤í—˜ ì„¤ê³„, í™˜ê²½ ì„¤ì •, ì‹œê°í™” ê·œê²©, ì‹¤í–‰ ë¡œê·¸, TODO ëª¨ë‘ ì´ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.

---

## 1. ì—°êµ¬ ëª©í‘œ

ë¬¸ì„œ/ì°¨íŠ¸ íŠ¹í™” ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ì—ì„œ ê¸°ì¡´ ViT ê¸°ë°˜ vision encoderì˜ êµ¬ì¡°ì  í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
**Structure-Factorized Attention (SFA)** + **Adaptive Density-Aware Tokenization (ADAT)** ì„ ì œì•ˆí•˜ê³ 
InternVL3.5-8B ìœ„ì—ì„œ ê²€ì¦í•œë‹¤.

**í•µì‹¬ ê°€ì„¤:**
1. ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ attentionì— structural bias (row/col/block)ë¥¼ ì£¼ì…í•˜ë©´ groundingì´ ì•ˆì •í™”ëœë‹¤
2. í…ìŠ¤íŠ¸ ë°€ì§‘ ì˜ì—­ì— í† í°ì„ ì§‘ì¤‘ ë°°ë¶„í•˜ë©´ ë™ì¼ budget ëŒ€ë¹„ ì •í™•ë„ê°€ í–¥ìƒëœë‹¤
3. ìœ„ ë‘ ë°©ë²•ìœ¼ë¡œ attention entropyê°€ ê°ì†Œí•˜ê³  hallucinationì´ ì¤„ì–´ë“ ë‹¤

**ë°©ë²•ë¡  ìœ ì˜ë¯¸ì„± ì¡°ê±´ (Cross-Architecture Generalization):**
> SFA/ADATê°€ **íŠ¹ì • vision encoder + íŠ¹ì • LLM** ì¡°í•©ì—ì„œë§Œ íš¨ê³¼ê°€ ìˆë‹¤ë©´ noveltyê°€ ì•½í•¨.
> **ì–´ë–¤ vision encoder (InternViT, CLIP, SigLIP)** ì— ì ìš©í•˜ë”ë¼ë„,
> **ì–´ë–¤ LLM decoder (Qwen, LLaMA, InternLM)** ì™€ ê²°í•©í•˜ë”ë¼ë„
> document-centric ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤ì§ˆì  ì„±ëŠ¥ í–¥ìƒì´ ìˆì–´ì•¼ í•¨.
>
> â†’ Phase 5ì— cross-architecture ì‹¤í—˜ ì¶”ê°€ (Â§10 ì°¸ì¡°)

---

## 2. í™˜ê²½ ì„¤ì •

### 2.1 ëª¨ë¸ ì„¸íŒ…

| í•­ëª© | ê°’ |
|------|-----|
| Base Model | InternVL3.5-8B |
| ëª¨ë¸ ê²½ë¡œ (ì‹¤ì œ ì‚¬ìš©) | **`/NetDisk/j_son/internvl_35/`** (48GB, conversation.py í¬í•¨) |
| ëª¨ë¸ ê²½ë¡œ (ì›ë³¸) | `/NetDisk/j_son/Model_original/InternVL_35` |
| Vision Encoder | InternViT-300M-448px (24 layers, dim=1024, 16 heads) |
| Patch Config | 448px / 14px patch = **32Ã—32 grid + 1 CLS = 1025 tokens** |
| LLM | InternLM2.5-7B-Chat (frozen) |
| ìˆ˜ì • ë²”ìœ„ | Vision encoder attention + projectorë§Œ í•™ìŠµ |
| ëª¨ë¸ ë¡œë”© | `AutoModel.from_config()` + safetensors ìˆ˜ë™ ë¡œë”© (**`from_pretrained` ì‚¬ìš© ë¶ˆê°€ â€” meta tensor ì´ìŠˆ**) |

### 2.2 GPU / Software

| í•­ëª© | ê°’ |
|------|-----|
| GPU | NVIDIA A100-PCIE-40GB Ã— 2 |
| CUDA | 12.1 |
| ì¶”ë¡  dtype | bfloat16 |
| Python | 3.10.19 |
| PyTorch | 2.4.0+cu121 |
| transformers | 5.1.0 |
| flash_attn | 2.8.3 |
| matplotlib | 3.10.8 |
| opencv | 4.11.0.86 (headless) |
| Conda env | `docmllm` |

```bash
conda activate docmllm
# ì¶”ê°€ ì„¤ì¹˜ (1íšŒ)
pip install seaborn scipy scikit-learn
```

### 2.3 ë¹„êµ ëª¨ë¸ (Ablation)

| # | ëª¨ë¸ | ì„¤ëª… |
|---|------|------|
| A | **Baseline** | Original InternVL3.5-8B (ìˆ˜ì • ì—†ìŒ) |
| B | **+ SFA only** | Attentionì— structural bias Ï†(s_i, s_j) ì¶”ê°€ |
| C | **+ ADAT only** | Adaptive density-aware tokenization ì ìš© |
| D | **+ SFA + ADAT** | ë‘ ëª¨ë“ˆ ë™ì‹œ ì ìš© |
| E | **+ Full (SFA + ADAT + SCR)** | Entropy/Stability regularization ì¶”ê°€ |

### 2.3.1 Cross-Architecture ê²€ì¦ ëª¨ë¸

SFAì˜ ë²”ìš©ì„±ì„ ì…ì¦í•˜ê¸° ìœ„í•´ ì•„ë˜ ì¡°í•©ì—ì„œë„ ì‹¤í—˜:

| Vision Encoder | LLM Decoder | ê¸°ë°˜ ëª¨ë¸ | ë¹„ê³  |
|----------------|-------------|-----------|------|
| InternViT-300M | InternLM2.5-7B | InternVL3.5-8B | **Primary (ìœ„ A~E)** |
| SigLIP-SO400M | Qwen2.5-7B | Qwen2.5-VL-7B | SigLIP + Qwen ì¡°í•© |
| CLIP-ViT-L/14 | Vicuna-7B / LLaMA3-8B | LLaVA-OV-7B | CLIP + LLaMA ê³„ì—´ |

**í•µì‹¬ ì‹¤í—˜**: ê° ì¡°í•©ì—ì„œ `Baseline` vs `+ SFA` ì˜ ChartQA Relaxed Accuracy ë¹„êµ
- 3ê°œ ëª¨ë¸ ëª¨ë‘ì—ì„œ SFA ì ìš© ì‹œ ì„±ëŠ¥ í–¥ìƒ â†’ **architecture-agnostic contribution** ì£¼ì¥ ê°€ëŠ¥
- 1ê°œ ì´ìƒì—ì„œ í–¥ìƒ ì—†ìŒ â†’ í•´ë‹¹ encoder íŠ¹ì„± ë¶„ì„ í•„ìš”

### 2.4 ê¸°ìˆ ì  ì´ìŠˆ í•´ê²° ê¸°ë¡

| ì´ìŠˆ | ì›ì¸ | í•´ê²° |
|------|------|------|
| `conversation.py` ëˆ„ë½ | `Model_original` ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì—†ìŒ | ëª¨ë¸ ê²½ë¡œë¥¼ `/NetDisk/j_son/internvl_35/`ë¡œ ë³€ê²½ |
| `torch_dtype` deprecated | transformers 5.xì—ì„œ ì œê±°ë¨ | `dtype` íŒŒë¼ë¯¸í„° ì‚¬ìš© |
| Meta tensor `RuntimeError` | `device_map="auto"` ì‹œ InternViTì˜ `torch.linspace().item()` í˜¸ì¶œ | `from_config()` + safetensors shard ìˆ˜ë™ ë¡œë”© (`model_utils.py`) |
| `model.chat()` API | `max_new_tokens`ê°€ keyword argê°€ ì•„ë‹˜ | `generation_config = dict(...)` í˜•íƒœë¡œ positional arg ì „ë‹¬ |
| Flash attention â†’ attention weight ë¯¸ë…¸ì¶œ | InternViT ê¸°ë³¸ê°’ì´ flash attn | `use_flash_attn=False` + QKV hookìœ¼ë¡œ ìˆ˜ë™ ê³„ì‚° |
| Patch grid 28 vs 32 | 448/14 = 32 (not 28) | ëª¨ë“  ëª¨ë“ˆì˜ `num_patches_h/w`ë¥¼ 32ë¡œ ìˆ˜ì • |
| CLS token ì²˜ë¦¬ | N=1025 = 1 CLS + 32Ã—32 spatial | structural biasë¥¼ `[:, :, 1:, 1:]`ì—ë§Œ ì ìš©, entropy ê³„ì‚° ì‹œ `[:, 1:]` |
| SFA forward ë°˜í™˜ê°’ ë¶ˆì¼ì¹˜ | InternAttention â†’ ë‹¨ì¼ í…ì„œ, SFA â†’ (out, attn) íŠœí”Œ | ë‹¨ì¼ í…ì„œ ë°˜í™˜, attnì€ `_last_attn_weights`ì— ì €ì¥ |
| `proj_drop` ëˆ„ë½ | InternAttentionì— ìˆìœ¼ë‚˜ SFAì— ì—†ìŒ | `nn.Dropout(0.0)` ì¶”ê°€ |
| VL loss gradient ëŠê¹€ | `model.forward()` ë‚´ë¶€ì—ì„œ `img_context_token_id` ë¯¸ì„¤ì • + token ìˆ˜ ë¶ˆì¼ì¹˜ | ìˆ˜ë™ `compute_vl_loss()` êµ¬í˜„: `.clone()` + `*0.0 + vit_embeds` gradient trick |
| `img_context_token_id` ë¯¸ì„¤ì • | `__init__`ì—ì„œ ë¯¸ì„¤ì •, `chat()`ì—ì„œë§Œ ë™ì  ì„¤ì • | `_resolve_img_context_token_id()` í—¬í¼ë¡œ tokenizerì—ì„œ ê²€ìƒ‰ í›„ model attribute ì„¤ì • |

---

## 3. ì‹¤í—˜ ë‹¨ê³„ë³„ ì„¤ê³„

### Step 0: Baseline Evaluation âœ… ì™„ë£Œ

**ëª©ì **: ìˆ˜ì • ì „ InternVL3.5-8Bì˜ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ìˆ˜ì¹˜ í™•ë³´
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/00_baseline_eval.py`

```bash
python experiments/scripts/00_baseline_eval.py \
    --model_path /NetDisk/j_son/internvl_35 \
    --output_dir experiments/results/00_baseline \
    --benchmarks chartqa --max_samples 200
```

**ê²°ê³¼**:
| ë²¤ì¹˜ë§ˆí¬ | Metric | Score | Samples | Time |
|----------|--------|-------|---------|------|
| ChartQA | Relaxed Accuracy | **0.6200** | 200 | 150.5s |

- Single-tile (448Ã—448) ì¶”ë¡ , dynamic_preprocess ë¯¸ì ìš©
- ì˜¤ë‹µ ì˜ˆì‹œ: ì†Œìˆ˜ì  ìˆ˜ì¹˜(0.57â†’10.04), ë¯¸ì„¸ ì°¨ì´(0.08â†’0.02) â€” **ìˆ«ì grounding ì·¨ì•½ì  í™•ì¸**

---

### Step 1: Text Density Estimator Training âœ… ì™„ë£Œ

**ëª©ì **: ë¬¸ì„œ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ë°€ì§‘ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” lightweight CNN í•™ìŠµ
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/01_density_estimator.py`

**Pseudo Label**: Canny edge + adaptive threshold â†’ Gaussian blur â†’ 28Ã—28 density map
**ëª¨ë¸**: 6-layer CNN (3â†’32â†’64â†’128â†’64â†’32â†’1), 186K params

```bash
python experiments/scripts/01_density_estimator.py \
    --mode train \
    --data_dirs "/NetDisk/juyeon/train/chartQA/ChartQA Dataset/train/png" /NetDisk/juyeon/train/dvqa/images \
    --output_dir experiments/results/01_density --epochs 10 --batch_size 64 --max_images 20000
```

**ê²°ê³¼**:
| í•­ëª© | ê°’ |
|------|-----|
| Train/Val | 19,000 / 1,000 |
| Best Val Loss | **0.001728** (Epoch 10) |
| ì‚°ì¶œë¬¼ | `best.pth`, `final.pth`, `visualizations/density_*.png` (20ì¥) |

---

### Step 2: SFA Module Test âœ… ì™„ë£Œ

**ëª©ì **: Structure-Factorized Attention ëª¨ë“ˆ ë‹¨ë… ë™ì‘ ê²€ì¦
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/02_sfa_module.py`

**SFA ìˆ˜ì‹**:
```
S_ij = (Q_i K_j^T) / sqrt(d) + Ï†(s_i, s_j)
Ï†(s_i, s_j) = w_rowÂ·[row_i==row_j] + w_colÂ·[col_i==col_j] + w_distÂ·(-manhattan(i,j)) + block_embed(b_i)^TÂ·block_embed(b_j)
```

**ê²°ê³¼**:
| í•­ëª© | ê°’ |
|------|-----|
| Forward | [2, 784, 1024] â†’ [2, 784, 1024] OK |
| Attention Entropy | 6.6089 |
| Structural Bias Params | **304** per layer (0.007%) |

---

### Step 3: SFA â†’ InternVL3.5 Integration âœ… ì™„ë£Œ

**ëª©ì **: InternViTì˜ self-attentionì„ SFAë¡œ êµì²´ í›„ inference ë™ì‘ í™•ì¸
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/03_sfa_integration.py`

**êµì²´ ì „ëµ**:
1. `InternVisionModel.encoder.layers[i].attn` â†’ `SFAAttention`
2. QKV/proj weight ë³µì‚¬ (pretrained ìœ ì§€), structural bias small init (std=0.02)
3. LLM frozen, vision encoder + projectorë§Œ trainable

**ê²°ê³¼**:
| í•­ëª© | ê°’ |
|------|-----|
| SFA êµì²´ layers | **24 / 24** (ì „ì²´) |
| Trainable | 337,590,400 / 8,528,325,760 (**4.0%**) |
| Inference | **PASSED** |

---

### Step 4: Attention Entropy Analysis (Baseline) âœ… ì™„ë£Œ

**ëª©ì **: text-dense vs sparse regionì˜ attention entropy ì°¨ì´ ì¸¡ì •
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/04_attention_analysis.py --mode entropy`

**ë°©ë²•**: flash attn ë¹„í™œì„±í™” â†’ QKV hookìœ¼ë¡œ ë§ˆì§€ë§‰ 4 layers ìº¡ì²˜ â†’ density map ê¸°ë°˜ region ë¶„ë¦¬

**ê²°ê³¼**:
| ì¸¡ì • ì˜ì—­ | Entropy |
|-----------|---------|
| Text-dense | **4.3322** |
| Sparse | **4.4377** |
| Ratio | **0.98x** |

â†’ **êµ¬ì¡°ì  ë°”ì´ì–´ìŠ¤ ë¶€ì¬ í™•ì¸** â€” SFA ì ìš© í›„ text region entropy ê°ì†Œ ì‹œ ê°€ì„¤ ê²€ì¦ ì„±ê³µ

---

### Step 5: Hallucination Rate Analysis (Baseline) âœ… ì™„ë£Œ

**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/04_attention_analysis.py --mode hallucination`

**ê²°ê³¼**:
| ì§€í‘œ | ê°’ |
|------|-----|
| Accuracy | **0.6500** |
| Hallucination Rate | **0.2550** (51/200) |
| Wrong Answer Rate | 0.0950 (19/200) |

â†’ ì˜¤ë¥˜ì˜ **73%ê°€ ìˆ«ì hallucination** â€” êµ¬ì¡°ì  grounding ë¶€ì¬ê°€ ì£¼ ì›ì¸

---

### Step 6: Token Efficiency Curve âœ… Placeholder ì™„ë£Œ

**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/04_attention_analysis.py --mode token_efficiency`
**ì‚°ì¶œë¬¼**: `token_efficiency_curve.{pdf,png}` â€” ADAT êµ¬í˜„ í›„ ì‹¤ ë°ì´í„°ë¡œ êµì²´ í•„ìš”

---

### Step 7: Attention Heatmap (Figure 3) â€” êµ¬í˜„ í•„ìš”

**ëª©ì **: Baseline vs SFA attention ë¶„í¬ë¥¼ ë™ì¼ ì´ë¯¸ì§€/ì§ˆì˜ì— ëŒ€í•´ ì‹œê°ì  ë¹„êµ
**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/06_attention_heatmap.py` (ì‹ ê·œ)

**ì…ë ¥**:
- ìƒ˜í”Œ: `experiments/figures/sample_images/chartqa_sample.png` (ì›ë³¸: `00339007006077.png`)
- ì§ˆì˜: `"What is the value for Haiti?"`
- Layers: 20, 21, 22, 23

**ë°©ë²•**:
1. `use_flash_attn = False` â†’ QKV hookìœ¼ë¡œ attention weight ìº¡ì²˜
2. Head í‰ê·  â†’ CLSâ†’spatial attention (0ë²ˆâ†’1:N) â†’ 32Ã—32 reshape
3. bilinear interpolation â†’ ì›ë³¸ í•´ìƒë„ upscale
4. `YlOrRd` colormap, alpha=0.5 ì˜¤ë²„ë ˆì´

```bash
# Baseline
python experiments/scripts/06_attention_heatmap.py \
    --model_type baseline --model_path /NetDisk/j_son/internvl_35 \
    --image experiments/figures/sample_images/chartqa_sample.png \
    --question "What is the value for Haiti?" --layers 20 21 22 23 \
    --output_dir experiments/figures/fig3_attention

# SFA (fine-tuning í›„)
python experiments/scripts/06_attention_heatmap.py \
    --model_type sfa --model_path /NetDisk/j_son/internvl_35 \
    --sfa_checkpoint experiments/results/03_sfa/sfa_weights.pth \
    --image experiments/figures/sample_images/chartqa_sample.png \
    --question "What is the value for Haiti?" --layers 20 21 22 23 \
    --output_dir experiments/figures/fig3_attention
```

---

### Step 8: Motivation Figure (Figure 1) â€” êµ¬í˜„ í•„ìš”

**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/05_figure_motivation.py` (ì‹ ê·œ)
**êµ¬ì„±** (1Ã—3): (a) Original + 14Ã—14 grid, (b) Density overlay, (c) Adaptive patching

```bash
python experiments/scripts/05_figure_motivation.py \
    --image experiments/figures/sample_images/chartqa_sample.png \
    --density_ckpt experiments/results/01_density/best.pth \
    --output_dir experiments/figures/fig1_motivation
```

---

### Step 9: Entropy Figure (Figure 5) â€” êµ¬í˜„ í•„ìš”

**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/07_figure_entropy.py` (ì‹ ê·œ)
**êµ¬ì„±** (1Ã—2): (a) Violin/box plot (text vs sparse), (b) Layer-wise entropy line plot (Baseline vs SFA)

```bash
python experiments/scripts/07_figure_entropy.py \
    --baseline_data experiments/results/04_analysis/entropy_analysis.json \
    --sfa_data experiments/results/05_sfa_analysis/entropy_analysis.json \
    --output_dir experiments/figures/fig5_entropy
```

---

### Step 10: Structural Bias ì‹œê°í™” (Figure 7) â€” SFA fine-tuning í›„

**ìŠ¤í¬ë¦½íŠ¸**: `experiments/scripts/09_structural_bias_viz.py` (ì‹ ê·œ)
**êµ¬ì„±** (1Ã—4): (a) Row bias, (b) Col bias, (c) Distance decay, (d) Combined Ï†

```bash
python experiments/scripts/09_structural_bias_viz.py \
    --sfa_checkpoint experiments/results/03_sfa/sfa_weights.pth \
    --grid_size 32 --output_dir experiments/figures/fig7_structural_bias
```

---

## 4. ë°ì´í„° ê²½ë¡œ

### í‰ê°€ ë°ì´í„°
| ë²¤ì¹˜ë§ˆí¬ | ê²½ë¡œ | Metric |
|----------|------|--------|
| ChartQA test | `/NetDisk/juyeon/train/chartQA/ChartQA Dataset/test/` | Relaxed Accuracy |
| DocVQA | `/NetDisk/juyeon/train/cauldron_data/docvqa/` | ANLS |
| PlotQA test | `/NetDisk/juyeon/train/plotqa/test/` | Relaxed Accuracy |
| DVQA val | `/NetDisk/juyeon/train/dvqa/val_easy_qa.json` | Exact Match |
| FigureQA test | `/NetDisk/juyeon/train/figureqa/no_annot_test1/` | Accuracy |
| OCRBench | HuggingFace `echo840/ocrbench` | Exact Match |

### í•™ìŠµ ë°ì´í„°
| ìš©ë„ | ë°ì´í„° | ê²½ë¡œ | ê·œëª¨ |
|------|--------|------|------|
| Density Estimator | ChartQA train | `/NetDisk/juyeon/train/chartQA/ChartQA Dataset/train/png/` | 18K |
| Density Estimator | DVQA | `/NetDisk/juyeon/train/dvqa/images/` | ~300K |
| Vision Fine-tuning | mllm_ready V3 | `/NetDisk/ingyu/VLM_DATA/mllm_ready/labels/V3/` | 5.5M rows |
| Vision Fine-tuning | Cauldron | `/NetDisk/juyeon/train/cauldron_data/` | 41GB |
| Vision Fine-tuning | LLaVA | `/NetDisk/juyeon/train/llava_data/` | 47GB |

---

## 5. ë…¼ë¬¸ Figure ìƒì„¸ ê³„íš

### 5.1 ì‹œê°í™” ê·œê²©

**ì¶œë ¥ í˜•ì‹**: PDF (í•™íšŒ ì œì¶œ) + PNG 300dpi (í”„ë¦¬ë·°)
**ì»¬ëŸ¼ í­**: Single 3.25in, Double 6.875in (ECCV)
**í°íŠ¸ í•˜í•œ**: 8pt, **ì„  ë‘ê»˜ í•˜í•œ**: 1pt

### 5.2 ìƒ‰ìƒí‘œ

| ì—­í•  | ì´ë¦„ | HEX | ìš©ë„ |
|------|------|-----|------|
| **Ours (SFA)** | Research Blue | `#1A73E8` | SFA ì»¤ë¸Œ, í•µì‹¬ ë¸”ë¡ |
| **êµ¬ì¡° ë°”ì´ì–´ìŠ¤** | Structural Teal | `#00897B` | Row/Col bias, Ï† í•¨ìˆ˜ |
| **ë°€ë„ ê°•ì¡°** | Density Amber | `#F9AB00` | ì •ë³´ ë°€ì§‘ ì§€ì—­ |
| **Baseline** | Neutral Slate | `#70757A` | Baseline ê²°ê³¼ |
| **ë°°ê²½** | Paper White | `#F8F9FA` | ê·¸ë˜í”„ ë°°ê²½ |
| **í…ìŠ¤íŠ¸** | Charcoal | `#202124` | ì¶• ë ˆì´ë¸” |
| + ADAT only | Coral | `#E8453C` | Ablation C |
| + SFA + ADAT | Purple | `#7B1FA2` | Ablation D |
| + Full (SCR) | Deep Blue | `#0D47A1` | Ablation E |
| Qwen2.5-VL | Pine Green | `#2E7D32` | ì™¸ë¶€ ë¹„êµ |
| LLaVA-OV | Brown | `#795548` | ì™¸ë¶€ ë¹„êµ |

### 5.3 íˆíŠ¸ë§µ Colormap

| ìš©ë„ | Colormap |
|------|----------|
| Attention íˆíŠ¸ë§µ | `YlOrRd` |
| Density map | `inferno` |
| Entropy ë¶„í¬ | `coolwarm` |
| Diff (SFA-Baseline) | `RdBu_r` |

### 5.4 matplotlib rcParams (ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ ê³µí†µ)

```python
import matplotlib; matplotlib.use("Agg")
import matplotlib.pyplot as plt

plt.rcParams.update({
    "font.family": "serif", "font.serif": ["DejaVu Serif", "STIXGeneral"],
    "font.sans-serif": ["DejaVu Sans"], "font.size": 10, "mathtext.fontset": "stix",
    "axes.titlesize": 14, "axes.labelsize": 12, "axes.linewidth": 0.8,
    "axes.edgecolor": "#202124", "axes.labelcolor": "#202124", "axes.facecolor": "#F8F9FA",
    "xtick.labelsize": 10, "ytick.labelsize": 10,
    "xtick.direction": "in", "ytick.direction": "in",
    "legend.fontsize": 10, "legend.framealpha": 0.9, "legend.edgecolor": "#CCCCCC",
    "grid.alpha": 0.3, "grid.linewidth": 0.5,
    "savefig.dpi": 300, "savefig.bbox": "tight", "savefig.pad_inches": 0.05,
    "figure.facecolor": "white",
})

COLORS = {
    "ours": "#1A73E8", "struct": "#00897B", "density": "#F9AB00",
    "baseline": "#70757A", "bg": "#F8F9FA", "text": "#202124",
    "adat": "#E8453C", "sfa_adat": "#7B1FA2", "full": "#0D47A1",
    "qwen": "#2E7D32", "llava": "#795548",
}
```

### 5.5 ìƒ˜í”Œ ì´ë¯¸ì§€

| íŒŒì¼ | ê²½ë¡œ | íŠ¹ì§• |
|------|------|------|
| `chartqa_sample.png` | `experiments/figures/sample_images/` | ìˆ˜í‰ ë°”ì°¨íŠ¸, 5ê°œêµ­ ë¹„êµ, ì¶•/ë ˆì´ë¸”/ìˆ˜ì¹˜ í¬í•¨ |

ì›ë³¸: `/NetDisk/juyeon/train/chartQA/ChartQA Dataset/test/png/00339007006077.png`

### 5.6 Figure â†” ì‹¤í—˜ ë§¤í•‘

| Figure | ë‚´ìš© | ìŠ¤í¬ë¦½íŠ¸ | ìƒíƒœ |
|--------|------|---------|------|
| **Fig 1**: Motivation | Uniform vs Adaptive patching 3-panel | `05_figure_motivation.py` | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** |
| **Fig 2**: Architecture | íŒŒì´í”„ë¼ì¸ íë¦„ë„ | draw.io / TikZ | ìˆ˜ë™ |
| **Fig 3**: Attention Heatmap | Baseline vs SFA ì˜¤ë²„ë ˆì´ ë¹„êµ | `06_attention_heatmap.py` | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** (Baseline ì‹¤í–‰ ëŒ€ê¸°) |
| **Fig 4**: Density Map | Original / Pseudo GT / Predicted | `01_density_estimator.py` | **ë°ì´í„° ì™„ë£Œ** (20ì¥) |
| **Fig 5**: Entropy | Text vs Sparse ë¶„í¬ + layer-wise | `07_figure_entropy.py` | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** (Baseline ì‹¤í–‰ ëŒ€ê¸°) |
| **Fig 6**: Token Efficiency | Budget sweep curve | `08_token_efficiency.py` | Placeholder ì™„ë£Œ |
| **Fig 7**: Structural Bias | Ï† í–‰ë ¬ 4-panel (row/col/dist/combined) | `09_structural_bias_viz.py` | SFA fine-tuning í›„ |
| **Fig 8**: Cross-Arch | Encoderë³„ SFA íš¨ê³¼ Bar chart | `12_cross_arch.py` | ğŸ“‹ Phase 5 |

---

## 6. ë…¼ë¬¸ Table ìƒì„¸ ê³„íš

### Table 1 â€” Main Results (InternVL3.5 Ablation)

| ëª¨ë¸ | ChartQA | DocVQA | TextVQA | OCRBench | AI2D |
|------|---------|--------|---------|----------|------|
| InternVL2.5-8B | (ê³µê°œ ìˆ˜ì¹˜) | | | | |
| Qwen2.5-VL-7B | (ê³µê°œ ìˆ˜ì¹˜) | | | | |
| LLaVA-OV-7B | (ê³µê°œ ìˆ˜ì¹˜) | | | | |
| **InternVL3.5-8B (Baseline)** | **0.620** | TBD | TBD | TBD | TBD |
| + SFA | TBD | | | | |
| + ADAT | TBD | | | | |
| + SFA + ADAT | TBD | | | | |
| **+ Full (Ours)** | TBD | | | | |

### Table 1b â€” Cross-Architecture Generalization

| Vision Encoder â†’ LLM | Baseline | + SFA | Î” |
|----------------------|----------|-------|---|
| InternViT â†’ InternLM2.5 (InternVL3.5) | 0.620 | TBD | TBD |
| SigLIP â†’ Qwen2.5 (Qwen2.5-VL) | TBD | TBD | TBD |
| CLIP-ViT-L â†’ LLaMA3 (LLaVA-OV) | TBD | TBD | TBD |

> 3ê°œ ì´ìƒì˜ encoder-decoder ì¡°í•©ì—ì„œ ì¼ê´€ëœ í–¥ìƒ â†’ "SFA is architecture-agnostic"

### Table 2 â€” Ablation Study

| ëª¨ë“ˆ | ChartQA | DocVQA | Halluc Rate | Params (ì¶”ê°€) |
|------|---------|--------|-------------|--------------|
| Baseline | 0.620 | TBD | 25.5% | +0 |
| + SFA | TBD | TBD | TBD | +7.3K (304/layer Ã— 24) |
| + ADAT | TBD | TBD | TBD | +186K |
| + SFA + ADAT | TBD | TBD | TBD | +7.3K + 186K |
| + Full (SFA+ADAT+SCR) | TBD | TBD | TBD | ìœ„ì™€ ë™ì¼ (SCRì€ lossë§Œ) |

### Table 3 â€” Structural Component Study

| Row | Col | Dist | Block | ChartQA | Halluc Rate |
|-----|-----|------|-------|---------|-------------|
| | | | | 0.620 | 25.5% |
| v | | | | TBD | TBD |
| | v | | | TBD | TBD |
| v | v | | | TBD | TBD |
| v | v | v | | TBD | TBD |
| v | v | v | v | TBD | TBD |

### Table 4 â€” Computational Cost

| ëª¨ë¸ | FLOPs (G) | Latency (ms) | Vision Tokens | Total Params |
|------|-----------|-------------|---------------|-------------|
| Baseline | TBD | TBD | 1024 | 8.5B |
| + SFA | TBD | TBD | 1024 | 8.5B + 7.3K |
| + ADAT (N=512) | TBD | TBD | 512 | 8.5B + 186K |

ì¸¡ì •: `fvcore.nn.FlopCountAnalysis`, `torch.cuda.Event` ê¸°ë°˜

---

## 7. Appendix ì‹œê°í™”

### 7.1 Hallucination Case Study
- 2Ã—4 grid: Baseline í‹€ë¦¬ê³  SFA ë§ì¶”ëŠ” 4 cases
- ìŠ¤í¬ë¦½íŠ¸: `10_hallucination_cases.py`
- Baseline ë°ì´í„°: `results/04_analysis/hallucination_analysis.json` (51ê±´ ìˆ«ì hallucination)

### 7.2 Multi-Resolution Scaling
- Line plot: í•´ìƒë„ {224, 448, 896, 1344} Ã— Baseline vs SFA+ADAT

### 7.3 Density Map Gallery
- 4Ã—5 grid, ë‹¤ì–‘í•œ ë¬¸ì„œ ìœ í˜•
- **ì´ë¯¸ ìƒì„±ë¨**: `results/01_density/visualizations/density_000~019.png`

---

## 8. ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

| # | ìŠ¤í¬ë¦½íŠ¸ | ìš©ë„ | ìƒíƒœ |
|---|---------|------|------|
| -- | `model_utils.py` | ê³µìš© ëª¨ë¸ ë¡œë”© (meta tensor íšŒí”¼) | âœ… **ì™„ë£Œ** |
| -- | `run_all.sh` | ì „ì²´ íŒŒì´í”„ë¼ì¸ | âœ… **ì™„ë£Œ** |
| 00 | `00_baseline_eval.py` | Baseline ë²¤ì¹˜ë§ˆí¬ í‰ê°€ | âœ… **ì‹¤í–‰ ì™„ë£Œ** |
| 01 | `01_density_estimator.py` | Density Estimator í•™ìŠµ/ì‹œê°í™” | âœ… **ì‹¤í–‰ ì™„ë£Œ** |
| 02 | `02_sfa_module.py` | SFA ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | âœ… **ì‹¤í–‰ ì™„ë£Œ** |
| 03a | `03_sfa_integration.py` | SFAâ†’InternVL í†µí•© | âœ… **ì‹¤í–‰ ì™„ë£Œ** |
| 03b | `03_sfa_finetune.py` | **SFA Fine-tuning** (train/eval) | âœ… **ì™„ë£Œ** (test passed, ì‹¤í–‰ ëŒ€ê¸°) |
| 04 | `04_attention_analysis.py` | Entropy/Hallucination/Token eff. | âœ… **ì‹¤í–‰ ì™„ë£Œ** |
| 05 | `05_figure_motivation.py` | Figure 1: Motivation | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** (ì‹¤í–‰ ëŒ€ê¸°) |
| 06 | `06_attention_heatmap.py` | Figure 3: Attention íˆíŠ¸ë§µ | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** (ì‹¤í–‰ ëŒ€ê¸°) |
| 07 | `07_figure_entropy.py` | Figure 5: Entropy ê·¸ë˜í”„ | âœ… **ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ** (ì‹¤í–‰ ëŒ€ê¸°) |
| 08 | `08_token_efficiency.py` | Figure 6: Token efficiency | **êµ¬í˜„ í•„ìš”** |
| 09 | `09_structural_bias_viz.py` | Figure 7: Ï† ì‹œê°í™” | **êµ¬í˜„ í•„ìš”** |
| 10 | `10_hallucination_cases.py` | Appendix: Case study | **êµ¬í˜„ í•„ìš”** |
| 11 | `11_adat_module.py` | ADAT ëª¨ë“ˆ | **êµ¬í˜„ í•„ìš”** |
| 12 | `12_cross_arch.py` | Cross-Architecture ì‹¤í—˜ | ğŸ“‹ Phase 5 |

---

## 9. ì‹¤í–‰ ë¡œê·¸

### 2026-02-20 â€” Baseline ì „ì²´ ë¶„ì„ ì™„ë£Œ

| Step | ì‹¤í—˜ | í•µì‹¬ ê²°ê³¼ |
|------|------|----------|
| 0 | Baseline Eval | ChartQA Acc = **0.620** (200 samples) |
| 1 | Density Estimator | Val Loss = **0.00173**, 186K params |
| 2 | SFA Module Test | Forward OK, bias params = 304 |
| 3 | SFA Integration | 24 layers, trainable **4.0%** |
| 4 | Entropy Analysis | Text/Sparse = **0.98x** (êµ¬ì¡°ì  ë°”ì´ì–´ìŠ¤ ë¶€ì¬) |
| 5 | Hallucination | Rate = **25.5%**, ì˜¤ë¥˜ì˜ 73%ê°€ ìˆ«ì hallucination |
| 6 | Token Efficiency | Placeholder ìƒì„± |

**í•µì‹¬ ë°œê²¬**:
1. Text-dense/sparse ê°„ entropy ì°¨ì´ ê±°ì˜ ì—†ìŒ (0.98x) â†’ **SFA í•„ìš”ì„± ì…ì¦**
2. Baseline ì˜¤ë¥˜ì˜ 73%ê°€ ìˆ«ì hallucination â†’ **êµ¬ì¡°ì  grounding ê°•í™” í•„ìš”**

**ìƒì„±ëœ íŒŒì¼**:
```
experiments/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ 00_baseline/summary.json
â”‚   â”œâ”€â”€ 01_density/best.pth, final.pth, visualizations/ (20ì¥)
â”‚   â””â”€â”€ 04_analysis/entropy_analysis.json, hallucination_analysis.json, token_efficiency_curve.{pdf,png}
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 03_sfa_finetune.py       (P2-1, êµ¬í˜„ ì¤‘)
â”‚   â”œâ”€â”€ 05_figure_motivation.py  (P1-1, ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ)
â”‚   â”œâ”€â”€ 06_attention_heatmap.py  (P1-2, ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ)
â”‚   â””â”€â”€ 07_figure_entropy.py     (P1-3, ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ)
â””â”€â”€ figures/
    â””â”€â”€ sample_images/chartqa_sample.png
```

### 2026-02-20 (ì˜¤í›„) â€” Phase 1 ìŠ¤í¬ë¦½íŠ¸ + P2-1 ì§„í–‰

| ì‘ì—… | ìƒíƒœ | ë¹„ê³  |
|------|------|------|
| P1-1 `05_figure_motivation.py` | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ | 3-panel (uniform/density/adaptive) |
| P1-2 `06_attention_heatmap.py` | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ | extract + compose ëª¨ë“œ |
| P1-3 `07_figure_entropy.py` | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ | violin + layer-wise 2-panel |
| P2-1 `03_sfa_finetune.py` | âœ… ì™„ë£Œ | gradient fix ì™„ë£Œ, test passed (loss 4.78â†’3.08) |

**P2-1 ê¸°ìˆ  ì´ìŠˆ â†’ âœ… í•´ê²°**:
- âŒ ì´ˆê¸°: `model.forward()` ì§ì ‘ í˜¸ì¶œ ì‹œ `img_context_token_id` ë¯¸ì„¤ì • + token ìˆ˜ ë¶ˆì¼ì¹˜
- âœ… ìˆ˜ì •: `compute_vl_loss()`ë¥¼ ìˆ˜ë™ êµ¬í˜„ (`.clone()` + `*0.0 + vit_embeds` gradient trick)
- âœ… `_resolve_img_context_token_id()` í—¬í¼ë¡œ model attribute ì¼ê´€ì„± ë³´ì¥
- âœ… **Test PASSED**: 10 steps, loss 4.78 â†’ 3.08 ê°ì†Œ, structural bias weights ì—…ë°ì´íŠ¸ í™•ì¸
  - Layer 0 w_row = [0.018, -0.009, -0.002, 0.015] (non-zero â†’ í•™ìŠµ ë™ì‘ ì •ìƒ)

---

## 10. ì•ìœ¼ë¡œ í•´ì•¼ í•  ì‹¤í—˜ (TODO)

### Phase 1: ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ âœ… ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ ì™„ë£Œ

| ID | ì‘ì—… | ì˜ì¡´ì„± | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|----|------|--------|--------|------|
| P1-1 | `05_figure_motivation.py` ì‹¤í–‰ | density ckpt âœ… | Figure 1 | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ, ì‹¤í–‰ ëŒ€ê¸° |
| P1-2 | `06_attention_heatmap.py` Baseline ì‹¤í–‰ | ëª¨ë¸ âœ… | Figure 3 (Baseline half) | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ, ì‹¤í–‰ ëŒ€ê¸° |
| P1-3 | `07_figure_entropy.py` Baseline ì‹¤í–‰ | entropy data âœ… | Figure 5 (Baseline half) | âœ… ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ, ì‹¤í–‰ ëŒ€ê¸° |

### Phase 2: SFA Fine-tuning (í•µì‹¬) â€” ğŸ”§ ì§„í–‰ ì¤‘

| ID | ì‘ì—… | ì˜ì¡´ì„± | ì‹œê°„ | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|----|------|--------|------|--------|------|
| P2-1 | **SFA fine-tuning ìŠ¤í¬ë¦½íŠ¸** | Step 3 âœ… | 2ì‹œê°„ | `03_sfa_finetune.py` | âœ… **ì™„ë£Œ** (test passed) |
| P2-2 | **SFA fine-tuning ì‹¤í–‰** (ChartQA train) | P2-1 | **~24h** | `results/03_sfa_train/` | â¬œ |
| P2-3 | SFA ëª¨ë¸ ChartQA eval | P2-2 | 1ì‹œê°„ | Table 1 "+SFA" | â¬œ |
| P2-4 | SFA entropy ì¬ì¸¡ì • | P2-2 | 2ì‹œê°„ | Figure 5 ì™„ì„± | â¬œ |
| P2-5 | SFA hallucination ì¬ì¸¡ì • | P2-2 | 2ì‹œê°„ | Table 2 "+SFA" | â¬œ |
| P2-6 | SFA attention heatmap | P2-2 | 1ì‹œê°„ | Figure 3 ì™„ì„± | â¬œ |
| P2-7 | Structural bias ì‹œê°í™” | P2-2 | 30ë¶„ | Figure 7 | â¬œ |
| P2-8 | Structural component ablation | P2-2 | 12ì‹œê°„ | Table 3 | â¬œ |

### Phase 3: ADAT êµ¬í˜„ + í†µí•©

| ID | ì‘ì—… | ì˜ì¡´ì„± | ì‹œê°„ | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|----|------|--------|------|--------|------|
| P3-1 | **ADAT ëª¨ë“ˆ êµ¬í˜„** | Density Est. âœ… | 4ì‹œê°„ | `11_adat_module.py` | â¬œ |
| P3-2 | ADAT ë‹¨ë… eval | P3-1 | 2ì‹œê°„ | Table 2 "+ADAT" | â¬œ |
| P3-3 | **SFA+ADAT fine-tuning** | P2-2 + P3-1 | **~24h** | `results/04_sfa_adat/` | â¬œ |
| P3-4 | Token efficiency ì‹¤ì¸¡ | P3-1 | 6ì‹œê°„ | Figure 6 ì‹¤ ë°ì´í„° | â¬œ |
| P3-5 | SFA+ADAT eval | P3-3 | 2ì‹œê°„ | Table 1, 2 "+SFA+ADAT" | â¬œ |

### Phase 4: Full System (SCR)

| ID | ì‘ì—… | ì˜ì¡´ì„± | ì‹œê°„ | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|----|------|--------|------|--------|------|
| P4-1 | **SCR loss êµ¬í˜„** | P3-3 | 2ì‹œê°„ | loss í•¨ìˆ˜ | â¬œ |
| P4-2 | **Full fine-tuning** | P4-1 | **~24h** | `results/05_full/` | â¬œ |
| P4-3 | 6ê°œ benchmark eval | P4-2 | 6ì‹œê°„ | Table 1 "Full" row | â¬œ |
| P4-4 | Compute cost ì¸¡ì • | P4-2 | 1ì‹œê°„ | Table 4 | â¬œ |
| P4-5 | Hallucination case study | P4-2 | 1ì‹œê°„ | Appendix | â¬œ |

### Phase 5: í™•ì¥ + Cross-Architecture + ë…¼ë¬¸ â­ NEW

> **ë°©ë²•ë¡  ìœ ì˜ë¯¸ì„±ì„ ìœ„í•œ í•µì‹¬ ì‹¤í—˜ Phase**
> SFAê°€ InternVLë¿ ì•„ë‹ˆë¼ ë‹¤ë¥¸ encoder-decoder ì¡°í•©ì—ì„œë„ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì•¼ í•¨

| ID | ì‘ì—… | ì˜ì¡´ì„± | ì‹œê°„ | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|----|------|--------|------|--------|------|
| P5-1 | ì¶”ê°€ benchmark (TextVQA, OCRBench, AI2D) | P4-2 | 4ì‹œê°„ | Table 1 ë‚˜ë¨¸ì§€ | â¬œ |
| P5-2 | ì™¸ë¶€ ëª¨ë¸ ê³µê°œ ìˆ˜ì¹˜ ì¡°ì‚¬ | - | 2ì‹œê°„ | Table 1 ë¹„êµ rows | â¬œ |
| P5-3 | Multi-resolution ì‹¤í—˜ | P4-2 | 4ì‹œê°„ | Appendix | â¬œ |
| P5-4 | Architecture diagram | - | ìˆ˜ë™ | Figure 2 | â¬œ |
| **P5-5** | **SFA â†’ Qwen2.5-VL (SigLIP+Qwen) ì ìš©** | P2-1 | **~24h** | Table 1b row 2 | â¬œ |
| **P5-6** | **SFA â†’ LLaVA-OV (CLIP+LLaMA) ì ìš©** | P2-1 | **~24h** | Table 1b row 3 | â¬œ |
| P5-7 | Cross-architecture ê²°ê³¼ ì¢…í•© + Figure 8 | P5-5, P5-6 | 2ì‹œê°„ | Fig 8 Bar chart | â¬œ |
| P5-8 | ë…¼ë¬¸ ì‘ì„± | P4-3, P5-7 | - | `eccv2026submission.tex` | â¬œ |

**Cross-Architecture ì‹¤í—˜ ì „ëµ**:
1. SFA ëª¨ë“ˆì€ **vision encoderì˜ self-attentionì—ë§Œ ì ìš©** â†’ encoderê°€ ë‹¬ë¼ì ¸ë„ QKV êµ¬ì¡°ê°€ ìˆìœ¼ë©´ ì ìš© ê°€ëŠ¥
2. SigLIPì˜ attention êµ¬ì¡°: í‘œì¤€ multi-head self-attention â†’ SFA ì§ì ‘ ì ìš© ê°€ëŠ¥
3. CLIP-ViT-L/14: ì—­ì‹œ í‘œì¤€ ViT attention â†’ SFA ì ìš© ê°€ëŠ¥
4. ê° ëª¨ë¸ë³„ `patch_*_with_sfa()` í•¨ìˆ˜ë¥¼ ë³„ë„ êµ¬í˜„í•˜ë˜, `SFAAttention` ëª¨ë“ˆ ìì²´ëŠ” ê³µìœ 
5. Decoder (LLM)ëŠ” frozenìœ¼ë¡œ ë™ì¼ â€” SFAì˜ íš¨ê³¼ê°€ vision ë‹¨ì—ì„œ ë°œìƒí•¨ì„ ì…ì¦

---

## 11. íƒ€ì„ë¼ì¸

```
2026-02-20 [ì™„ë£Œ] Step 0~6: Baseline ë¶„ì„ ì „ì²´ ì™„ë£Œ
2026-02-20 [ì™„ë£Œ] Phase 1: ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ (Figure 1, 3, 5)
2026-02-20 [ì™„ë£Œ] Phase 2-1: SFA fine-tuning ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ + test passed
2026-02-20~21    Phase 2-2: SFA fine-tuning ì‹¤í–‰ ì‹œì‘ (~24h) â† â­ ë‹¤ìŒ ë‹¨ê³„
2026-02-22       Phase 2-2 ì™„ë£Œ â†’ Phase 2-3~8: SFA í›„ì† ë¶„ì„
2026-02-23       Phase 3-1: ADAT êµ¬í˜„
2026-02-23~24    Phase 3-3: SFA+ADAT fine-tuning (~24h)
2026-02-24       Phase 3-4,5: Token efficiency + eval
2026-02-25       Phase 4: SCR + Full fine-tuning (~24h)
2026-02-26       Phase 4-3~5: Full eval + compute + case study
2026-02-27~28    Phase 5-5,6: Cross-Architecture ì‹¤í—˜ (Qwen2.5-VL, LLaVA-OV)  â­ NEW
2026-03-01~      Phase 5-8: ë…¼ë¬¸ ì‘ì„±
```

### Cross-Architecture ì‹¤í—˜ ì¤‘ìš”ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë…¼ë¬¸ Story                                           â”‚
â”‚                                                      â”‚
â”‚ 1. SFAëŠ” vision encoderì˜ attentionì— structural    â”‚
â”‚    biasë¥¼ ì£¼ì…í•˜ëŠ” ë²”ìš© ëª¨ë“ˆì´ë‹¤                       â”‚
â”‚                                                      â”‚
â”‚ 2. InternViTì—ì„œ íš¨ê³¼ í™•ì¸ (Primary)                  â”‚
â”‚                                                      â”‚
â”‚ 3. SigLIP, CLIP ë“± ë‹¤ë¥¸ encoderì—ì„œë„ ë™ì¼ íš¨ê³¼ í™•ì¸   â”‚
â”‚    â†’ architecture-agnostic contribution               â”‚
â”‚                                                      â”‚
â”‚ 4. Decoder (Qwen, LLaMA, InternLM) ë³€ê²½ì—ë„ robust    â”‚
â”‚    â†’ encoder-side improvementê°€ decoderì— ì „ì´ë¨ ì…ì¦  â”‚
â”‚                                                      â”‚
â”‚ â‡’ "SFAëŠ” ViT attentionì˜ ê·¼ë³¸ì  í•œê³„ë¥¼ í•´ê²°í•˜ëŠ”        â”‚
â”‚    ë²”ìš© ëª¨ë“ˆ" ì´ë¼ëŠ” ì£¼ì¥ì´ ê°€ëŠ¥                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
