# Structure-Factorized Attention (SFA) for Document-Centric Multimodal LLMs

## ë°œí‘œìë£Œ

---

## Slide 1. ì—°êµ¬ ë°°ê²½ ë° ë¬¸ì œ ì •ì˜

### ê¸°ì¡´ Vision Encoderì˜ í•œê³„

í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ ë©€í‹°ëª¨ë‹¬ LLM (InternVL, Qwen-VL, LLaVA ë“±)ì€ **ViT ê¸°ë°˜ vision encoder**ë¥¼ ì‚¬ìš©.

**ë¬¸ì œì :**

- **Uniform Patch Tokenization**: ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë°€ì§‘ ì˜ì—­ê³¼ ë¹ˆ ê³µê°„ì— ë™ì¼í•œ í¬ê¸°ì˜ íŒ¨ì¹˜ ì ìš©
  - í…ìŠ¤íŠ¸ ì˜ì—­: ì •ë³´ê°€ ê³¼ë„í•˜ê²Œ ì••ì¶• â†’ ì‘ì€ ìˆ«ì/í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ë¥˜
  - ë¹ˆ ê³µê°„: ë¶ˆí•„ìš”í•œ í† í° ë‚­ë¹„

- **Layout Inductive Bias ë¶€ì¬**: ViTì˜ positional encodingì€ ë¬¸ì„œì˜ í–‰/ì—´/ë¸”ë¡ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨

- **ìˆ«ì Hallucination**: ì°¨íŠ¸/í‘œì—ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìˆ«ìë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ì‹¬ê°

### í•µì‹¬ ì—°êµ¬ ì§ˆë¬¸

> ë¬¸ì„œ ì´ë¯¸ì§€ì˜ **êµ¬ì¡°ì  íŠ¹ì„±**ì„ vision encoderì˜ attentionì— ì§ì ‘ ì£¼ì…í•˜ë©´,
> groundingì´ ì•ˆì •í™”ë˜ê³  hallucinationì´ ì¤„ì–´ë“œëŠ”ê°€?

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 1: Motivation** â€” (a) Uniform grid (b) Density heatmap (c) Adaptive patching | `experiments/figures/fig1_motivation/figure1_motivation.png` | âœ… ì™„ë£Œ |
| **ChartQA ìƒ˜í”Œ ì´ë¯¸ì§€** â€” ë¬¸ì œ ìƒí™© ì˜ˆì‹œìš© ì°¨íŠ¸ ì´ë¯¸ì§€ | `experiments/figures/sample_images/chartqa_sample.png` | âœ… ì™„ë£Œ |

---

## Slide 2. Baseline ë¶„ì„ ê²°ê³¼

### ì‹¤í—˜ í™˜ê²½

| í•­ëª© | ê°’ |
|------|-----|
| Base Model | InternVL3.5-8B |
| Vision Encoder | InternViT-300M-448px (24 layers) |
| LLM | InternLM2.5-7B-Chat |
| GPU | NVIDIA A100-PCIE-40GB Ã— 2 |
| í‰ê°€ ë²¤ì¹˜ë§ˆí¬ | ChartQA (Relaxed Accuracy) |

### Baseline ì„±ëŠ¥

| Metric | Score |
|--------|-------|
| **ChartQA Relaxed Accuracy** | **0.620** |

### Hallucination ë¶„ì„ (200 samples)

| ë¶„ë¥˜ | ìˆ˜ | ë¹„ìœ¨ |
|------|-----|------|
| ì •ë‹µ | 130 | 65.0% |
| **ìˆ«ì Hallucination** | **51** | **25.5%** |
| ì˜¤ë‹µ (ê¸°íƒ€) | 19 | 9.5% |

> **ì˜¤ë¥˜ì˜ 73%ê°€ ìˆ«ì hallucination** â€” ì´ë¯¸ì§€ì— ì—†ëŠ” ìˆ«ìë¥¼ ìƒì„±

### ì˜¤ë‹µ ì˜ˆì‹œ

| Question | GT | Prediction | ì˜¤ë¥˜ ìœ í˜• |
|----------|-----|-----------|-----------|
| "What was the largest dark red bar value?" | 0.08 | **26** | ìˆ«ì hallucination |
| "What is the difference between highest and lowest?" | 54 | **99** | ìˆ«ì hallucination |
| "What's the value for the rightmost bar?" | 0.57 | **10.04** | ìë¦¿ìˆ˜ ì˜¤ë¥˜ |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table: Baseline ì„±ëŠ¥** â€” ChartQA Relaxed Accuracy = 0.620 | `experiments/results/00_baseline/summary.json` | âœ… ì™„ë£Œ |
| **Table: Hallucination ë¶„ì„** â€” 200 samples ë¶„ë¥˜ (ì •ë‹µ/halluc/ì˜¤ë‹µ) | `experiments/results/04_analysis/hallucination_analysis.json` | âœ… ì™„ë£Œ |
| **Table: ì˜¤ë‹µ ì˜ˆì‹œ** â€” hallucination_analysis.jsonì—ì„œ ëŒ€í‘œ ì˜¤ë‹µ 3ê±´ ì¶”ì¶œ | `experiments/results/04_analysis/hallucination_analysis.json` | âœ… ì™„ë£Œ |

---

## Slide 3. Attention Entropy ë¶„ì„ (Baseline)

### Attention Entropy ë¶„ì„

| ì˜ì—­ | Entropy | ì°¨ì´ |
|------|---------|------|
| Text-dense region | 4.3322 | - |
| Sparse region | 4.4377 | - |
| **ë¹„ìœ¨** | **0.98x** | â‰ˆ ë™ì¼ |

> Text-dense/sparse ê°„ attention entropy ì°¨ì´ê°€ ê±°ì˜ ì—†ìŒ
> â†’ **Vision encoderê°€ ë¬¸ì„œ êµ¬ì¡°ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•¨** â†’ SFA í•„ìš”ì„± ì…ì¦

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 5: Entropy Analysis (Baseline)** â€” Violin plot + Layer-wise line plot | `experiments/figures/fig5_entropy/fig5_entropy.png` | âœ… ì™„ë£Œ |
| **Table: Entropy í†µê³„** â€” text-dense vs sparse ì˜ì—­ entropy ë¹„êµ | `experiments/results/04_analysis/entropy_analysis.json` | âœ… ì™„ë£Œ |
| **Figure 5: Entropy Analysis (Baseline vs SFA)** â€” Violin + Layer-wise ë¹„êµ | `experiments/figures/fig5_entropy/fig5_entropy.png` | âœ… ì™„ë£Œ |

---

## Slide 4. ì œì•ˆ ë°©ë²•: Structure-Factorized Attention (SFA)

### í•µì‹¬ ì•„ì´ë””ì–´

ê¸°ì¡´ ViT self-attentionì— **ë¬¸ì„œ êµ¬ì¡° bias**ë¥¼ ì¶”ê°€:

```
ê¸°ì¡´:  S_ij = (Q_i Â· K_j^T) / âˆšd

ì œì•ˆ:  S_ij = (Q_i Â· K_j^T) / âˆšd  +  Ï†(s_i, s_j)
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               Content Attention    Structural Bias
```

### Structural Bias Ï†(s_i, s_j) êµ¬ì„±

```
Ï† = w_row Â· ğŸ™[row_i = row_j]           â† ê°™ì€ í–‰ íŒ¨ì¹˜ ê°„ ê°•í™”
  + w_col Â· ğŸ™[col_i = col_j]           â† ê°™ì€ ì—´ íŒ¨ì¹˜ ê°„ ê°•í™”
  + w_dist Â· (-manhattan(i,j))          â† ê°€ê¹Œìš´ íŒ¨ì¹˜ ê°„ ê°•í™”
  + block_embed(b_i)^T Â· block_embed(b_j)  â† ê°™ì€ ë¸”ë¡ ê°„ ê°•í™”
```

### íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±

| Component | Parameters | Overhead |
|-----------|-----------|----------|
| w_row (per head) | 16 | - |
| w_col (per head) | 16 | - |
| w_dist (per head) | 16 | - |
| block_embed (16 blocks Ã— 16 heads) | 256 | - |
| **Layerë‹¹ í•©ê³„** | **304** | **0.007%** |
| **ì „ì²´ (24 layers)** | **7,296** | **0.002%** |

> ì „ì²´ ëª¨ë¸ ëŒ€ë¹„ **0.002%ì˜ íŒŒë¼ë¯¸í„°**ë§Œ ì¶”ê°€í•˜ë©´ì„œ êµ¬ì¡°ì  inductive bias ì œê³µ

### ì ìš© ë°©ì‹

- InternViTì˜ **24ê°œ attention layer ëª¨ë‘**ì— SFA ì ìš©
- CLS í† í°ì—ëŠ” structural bias ë¯¸ì ìš© (spatial tokensë§Œ)
- Pretrained QKV weights ìœ ì§€, structural biasë§Œ small init (std=0.02)

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 2: Architecture Diagram** â€” ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë„ | `experiments/figures/fig2_architecture.png` | âœ… ì™„ë£Œ |
| **Table: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±** â€” SFA ì¶”ê°€ íŒŒë¼ë¯¸í„° ë¶„ì„ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |

---

## Slide 5. Adaptive Density-Aware Tokenization (ADAT)

### ë™ê¸°

ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ:
- í…ìŠ¤íŠ¸ ë°€ì§‘ ì˜ì—­ â†’ ë” ì‘ì€ íŒ¨ì¹˜ í•„ìš” (ë†’ì€ í•´ìƒë„)
- ë¹ˆ ê³µê°„ â†’ í° íŒ¨ì¹˜ë¡œ ì¶©ë¶„ (í† í° ì ˆì•½)

### Text Density Estimator

| í•­ëª© | ê°’ |
|------|-----|
| Architecture | 6-layer CNN |
| Parameters | **186K** (ê²½ëŸ‰) |
| Input | 448Ã—448 document image |
| Output | 28Ã—28 density heatmap D(x,y) âˆˆ [0,1] |
| Training | Self-supervised (pseudo labels) |
| Val Loss | **0.001728** |

### Pseudo Label ìƒì„±

```
Document Image â†’ Canny Edge Detection â†’ Adaptive Threshold â†’ Gaussian Blur â†’ Density Map
```

ë³„ë„ annotation ì—†ì´ ì´ë¯¸ì§€ ìì²´ì—ì„œ í…ìŠ¤íŠ¸ ë°€ë„ ì¶”ì • ê°€ëŠ¥

### Density-Guided Block Assignment (êµ¬í˜„ ì™„ë£Œ)

Density mapì„ 16ê°œ ë¸”ë¡ìœ¼ë¡œ ì–‘ìí™”í•˜ì—¬ SFAì˜ `block_embed`ì— ì „ë‹¬:

```
Density Map (28Ã—28) â†’ Resize (32Ã—32) â†’ Quantize (16 bins) â†’ block_ids [1024]
                                                              â†“
SFA block_embed(b_i)^T Â· block_embed(b_j): ê°™ì€ ë°€ë„ ë¸”ë¡ ë‚´ íŒ¨ì¹˜ ê°„ attention ê°•í™”
```

### SFA+ADAT í•™ìŠµ ê²°ê³¼

| í•­ëª© | ê°’ |
|------|-----|
| Trainable params | 7,296 (SFA structural bias only) |
| Strategy | Backbone frozen + Density-guided blocks |
| Training loss | 5.48 â†’ 5.07 (3 epochs) |
| Density blocks | 16 |

### SFA+ADAT vs SFA-only ì„±ëŠ¥ ë¹„êµ

| Model | ChartQA Acc | Halluc Rate |
|-------|------------|-------------|
| SFA-only (frozen) | 0.6244 | 20.2% |
| **SFA+ADAT (frozen)** | **0.6284** | **20.0%** |
| ë³€í™” | **+0.4%p** | **-0.2%p** |

> Density-guided block assignmentì´ ì†Œí­ ê°œì„ ì„ ë³´ì´ë‚˜, block_embed ìì²´ì˜ íŒŒë¼ë¯¸í„°ê°€ ì ì–´(256/layer) ê·¹ì ì¸ ì°¨ì´ëŠ” ì—†ìŒ

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 1: Motivation (b) Density Heatmap** â€” ë°€ë„ ì¶”ì • ê²°ê³¼ | `experiments/figures/fig1_motivation/figure1_motivation.png` (panel b) | âœ… ì™„ë£Œ |
| **Density Map ì‹œê°í™”** â€” 20ê°œ ChartQA ì´ë¯¸ì§€ì˜ density estimation ê²°ê³¼ | `experiments/results/01_density/visualizations/density_000~019.png` | âœ… ì™„ë£Œ |
| **Token Efficiency Curve** â€” ë°€ë„ ê¸°ë°˜ í† í° ì ˆì•½ íš¨ê³¼ ê·¸ë˜í”„ | `experiments/results/04_analysis/token_efficiency_curve.png` | âœ… ì™„ë£Œ |
| **SFA+ADAT í•™ìŠµ ë¡œê·¸** | `experiments/results/07_sfa_adat/train_log.json` | âœ… ì™„ë£Œ |
| **SFA+ADAT í‰ê°€ ê²°ê³¼** | `experiments/results/07_sfa_adat/eval/summary.json` | âœ… ì™„ë£Œ |

---

## Slide 6. í•™ìŠµ ì „ëµ ë° OOM í•´ê²°

### í•™ìŠµ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trainable (4.0%)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ InternViT-300M   â”‚  â”‚ MLP Projector    â”‚     â”‚
â”‚  â”‚ + SFA (24 layers)â”‚â†’â”‚ (4096â†’4096)      â”‚     â”‚
â”‚  â”‚ GPU 0: 8.1 GB    â”‚  â”‚                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                 â”‚               â”‚
â”‚  Frozen (96.0%)                 â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ InternLM2.5-7B-Chat (bf16)              â”‚   â”‚
â”‚  â”‚ GPU 1: 15.3 GB                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### OOM ë¬¸ì œ ë° í•´ê²°

**ë¬¸ì œ**: InternVL3.5-8B (17GB bf16)ì„ A100-40GBì—ì„œ fine-tuning ì‹œ OOM

**í•´ê²° (2-GPU Model Parallel):**

| ì „ëµ | ë‚´ìš© | íš¨ê³¼ |
|------|------|------|
| Vision â†’ GPU 0 | Vision encoder + projector (trainable) | 8.1 GB |
| LLM â†’ GPU 1 | LLM (frozen, bf16 full) | 15.3 GB |
| Gradient Checkpointing | Vision encoder í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê° | -50% activation |
| batch_size=4 | GPU ì—¬ìœ  í™œìš© | ì²˜ë¦¬ëŸ‰ 4ë°°â†‘ |

**ì´ì „ ì‹œë„ (ì‹¤íŒ¨):**
- Single GPU full bf16 â†’ OOM (17GB + optimizer + activations > 40GB)
- DDP 2-GPU â†’ ê° rankë§ˆë‹¤ full model ë³µì‚¬ â†’ OOM

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **í•™ìŠµ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨** â€” 2-GPU ë°°ì¹˜ (ìœ„ ASCII ë‹¤ì´ì–´ê·¸ë¨ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ ë‹¤ì´ì–´ê·¸ë¨ | âœ… ì™„ë£Œ |
| **Table: OOM í•´ê²° ì „ëµ** â€” GPU ë¶„í•  + checkpointing íš¨ê³¼ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **Table: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** â€” ì‹¤ì¸¡ ê°’ | `experiments/results/03_sfa_train/train.log` (G0:8.1GB G1:15.3GB) | âœ… ì™„ë£Œ |

---

## Slide 7. SFA í•™ìŠµ ì§„í–‰ ìƒí™© ë° Loss ì¶”ì´

### í•™ìŠµ ì„¤ì •

| í•­ëª© | ê°’ |
|------|-----|
| Data | ChartQA train (28,299 samples) |
| Effective batch size | 4 Ã— 8 (grad_accum) = **32** |
| Epochs | 3 |
| Optimizer | AdamW (lr=2e-5, cosine schedule) |
| Total optimizer steps | 2,652 |

### Loss ì¶”ì´

```
Epoch 1:
  step   80 | loss: 4.7813  â† ì´ˆê¸°
  step  400 | loss: 4.3514
  step 2000 | loss: 1.3860
  step 7074 | loss: 1.1015  â† Epoch 1 ì™„ë£Œ

Epoch 2:
  step   80 | loss: 0.6696
  step 2000 | loss: 0.5361
  step 7074 | loss: 0.5088  â† Epoch 2 ì™„ë£Œ

Epoch 3:
  step   80 | loss: 0.5311
  step 2000 | loss: 0.4683
  step 7074 | loss: 0.4579  â† Epoch 3 ì™„ë£Œ (Best)
```

> **Loss: 4.78 â†’ 0.46 (ì•½ 90.4% ê°ì†Œ)** â€” 3 epochs í•™ìŠµ ì™„ë£Œ

### GPU í™œìš©ë¥ 

| GPU | ì—­í•  | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì—¬ìœ  |
|-----|------|-----------|------|
| GPU 0 | Vision (trainable) | 8.1 GB | 32.9 GB (80%) |
| GPU 1 | LLM (frozen) | 15.3 GB | 25.7 GB (63%) |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Loss Curve ê·¸ë˜í”„** â€” 264 step-level ë°ì´í„° (4.78â†’0.46, 90.4% ê°ì†Œ) | `experiments/figures/fig_loss_curve/loss_curve.png` | âœ… ì™„ë£Œ |
| **Table: í•™ìŠµ ì„¤ì •** â€” hyperparameter ìš”ì•½ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **Table: GPU í™œìš©ë¥ ** â€” ì‹¤ì¸¡ GPU ë©”ëª¨ë¦¬ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **í•™ìŠµ ë¡œê·¸ ë°ì´í„°** â€” stepë³„ loss/lr ê¸°ë¡ | `experiments/results/03_sfa_train/train_log.json` | âœ… ì™„ë£Œ |

---

## Slide 8. SFA ì„±ëŠ¥ í‰ê°€ ê²°ê³¼

### Table 1: ChartQA ì„±ëŠ¥ ë¹„êµ (Main Result)

| Model | ChartQA Relaxed Acc | ë³€í™” |
|-------|-------------------|------|
| InternVL3.5-8B (Baseline) | 0.620 | - |
| **+ SFA (full encoder ft)** | **0.509** | **-0.111** |

### Table 2: Hallucination ë¹„êµ (200 samples)

| ë¶„ë¥˜ | Baseline | + SFA | ë³€í™” |
|------|----------|-------|------|
| ì •ë‹µ | 130 (65.0%) | 105 (52.5%) | -12.5%p |
| ìˆ«ì Hallucination | 51 (25.5%) | 46 (23.0%) | **-2.5%p** |
| ì˜¤ë‹µ (ê¸°íƒ€) | 19 (9.5%) | 49 (24.5%) | +15.0%p |

### ë¶„ì„: Catastrophic Forgetting ë¬¸ì œ

> **ì „ì²´ vision encoder (300M params)ë¥¼ 28K ChartQAë§Œìœ¼ë¡œ fine-tuningí•˜ì—¬ ì„±ëŠ¥ í•˜ë½ ë°œìƒ**
>
> - Hallucination rateëŠ” ì†Œí­ ê°œì„  (25.5% â†’ 23.0%)
> - ê·¸ëŸ¬ë‚˜ ì „ì²´ ì •í™•ë„ í•˜ë½ (0.620 â†’ 0.509)ì´ ì‹¬ê°
> - ì›ì¸: pretrained ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì˜ catastrophic forgetting
>
> **ë‹¤ìŒ ì‹¤í—˜ ë°©í–¥:**
> - SFA structural biasë§Œ í•™ìŠµ (7,296 params), vision encoder backbone freeze
> - ë˜ëŠ” ë” ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„° ì‚¬ìš© (ChartQA + DocVQA + ê¸°íƒ€)

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table 1: ChartQA ì„±ëŠ¥ ë¹„êµ** â€” Baseline vs +SFA | `experiments/results/05_sfa_eval/eval_results.json` | âœ… ì™„ë£Œ |
| **Table 2: Hallucination ë¹„êµ** â€” Baseline vs +SFA 200-sample ë¶„ë¥˜ | `experiments/results/05_sfa_eval/hallucination_sfa.json` | âœ… ì™„ë£Œ |

---

## Slide 9. Entropy ë¶„ì„ â€” Baseline vs SFA

### Attention Entropy ë¹„êµ

| ì˜ì—­ | Baseline | + SFA | ë³€í™” |
|------|----------|-------|------|
| Text-dense region | 4.3322 | 4.7397 | +0.408 |
| Sparse region | 4.4377 | 4.7447 | +0.307 |
| Dense/Sparse Delta | -0.106 | **-0.005** | **Delta ì¶•ì†Œ** |

> - SFA ì ìš© í›„ ì „ì²´ entropyê°€ ì¦ê°€ (4.33 â†’ 4.74)
> - text/sparse ê°„ Deltaê°€ -0.106 â†’ -0.005ë¡œ ê±°ì˜ 0ì— ìˆ˜ë ´
> - ì´ëŠ” SFAê°€ êµ¬ì¡°ì  ì°¨ì´ë¥¼ ê· ì¼í™”í–ˆìœ¼ë‚˜, ì „ì²´ entropy ì¦ê°€ê°€ ì •ë³´ ì†ì‹¤ì„ ì‹œì‚¬
> - Vision encoder full fine-tuningìœ¼ë¡œ ì¸í•œ attention íŒ¨í„´ ë³€í™”

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 5: Entropy Analysis (Baseline vs SFA)** â€” Violin + Layer-wise ë¹„êµ | `experiments/figures/fig5_entropy/fig5_entropy.png` | âœ… ì™„ë£Œ (ì¬ìƒì„±ë¨) |
| **Table: Entropy ë¹„êµ** â€” Baseline vs SFA entropy í†µê³„ | `experiments/results/05_sfa_eval/entropy_analysis_sfa.json` | âœ… ì™„ë£Œ |

---

## Slide 10. Structural Bias ì‹œê°í™” & Attention Heatmap

### Figure 7: í•™ìŠµëœ Structural Bias

- 24ê°œ layer Ã— 16 headsì˜ w_row, w_col, w_dist ê°’ ì‹œê°í™”
- Layerë³„ bias magnitude ë¶„ì„
- ë§ˆì§€ë§‰ layerì˜ structural bias matrix ì‹œê°í™”

### Attention Map ë¹„êµ â€” Baseline vs SFA

ë™ì¼ ì°¨íŠ¸ ì´ë¯¸ì§€("What is the value for Haiti?")ì— ëŒ€í•œ attention heatmap ë¹„êµ:
- 4ê°œ layer (11, 17, 20, 23) Ã— 2 ëª¨ë¸ (Baseline, SFA) ë¹„êµ
- ë‘ ëª¨ë¸ ëª¨ë‘ ì •ë‹µ (6.12) ì¶œë ¥
- Layer 23ì—ì„œ attention íŒ¨í„´ ì°¨ì´ê°€ ê°€ì¥ ëšœë ·

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 7(a): Bias Heatmap** â€” w_row/w_col/w_dist per layer & head | `experiments/figures/fig7_structural_bias/fig7a_bias_heatmap.png` | âœ… ì™„ë£Œ |
| **Figure 7(b): Bias Bar** â€” Layer-wise mean magnitude | `experiments/figures/fig7_structural_bias/fig7b_bias_bar.png` | âœ… ì™„ë£Œ |
| **Figure 7(c): Bias Matrix** â€” Layer 23 structural bias matrix | `experiments/figures/fig7_structural_bias/fig7c_bias_matrix.png` | âœ… ì™„ë£Œ |
| **Figure 3: Attention Heatmap (Baseline vs SFA)** â€” 4 layers Ã— 2 models ë¹„êµ | `experiments/figures/fig3_attention_heatmap/fig3_attention_comparison.png` | âœ… ì™„ë£Œ |
| **Figure 3: Layer 23 Detail** â€” ìµœì¢… layer ìƒì„¸ ë¹„êµ | `experiments/figures/fig3_attention_heatmap/fig3_L23.png` | âœ… ì™„ë£Œ |

---

## Slide 11. Ablation Study â€” Backbone Freeze vs Full Fine-tuning

### Table 3: Training Strategy Ablation

| Configuration | Trainable Params | ChartQA Acc | Halluc Rate | ë¹„ê³  |
|--------------|-----------------|-------------|-------------|------|
| Baseline (no SFA) | 0 | 0.620 | 25.5% | Pretrained ê·¸ëŒ€ë¡œ |
| + SFA (full encoder ft) | 337M (4.0%) | 0.509 | 23.0% | Catastrophic Forgetting |
| + SFA-only (backbone frozen) | 7,296 (0.0%) | 0.6244 | 20.2% | ì •í™•ë„ ìœ ì§€ + Halluc ê°ì†Œ |
| + SFA+ADAT (backbone frozen) | 7,296 (0.0%) | 0.6284 | 20.0% | Density-guided blocks |
| **+ SFA+ADAT+SCR (backbone frozen)** | **7,296 (0.0%)** | **0.6288** | **20.0%** | **Best: + Entropy regularization** |

### í•µì‹¬ ë°œê²¬

> **SFA+ADAT+SCR (backbone frozen) ì „ëµì´ ìµœì :**
> - ì •í™•ë„: 0.6288 (baseline 0.620 ëŒ€ë¹„ **+1.4%**, forgetting ì—†ìŒ)
> - Hallucination: 20.0% (baseline 25.5% ëŒ€ë¹„ **-5.5%p ê°ì†Œ**)
> - **ë‹¨ 7,296ê°œ íŒŒë¼ë¯¸í„°**ë§Œ í•™ìŠµ (ì „ì²´ì˜ 0.0%)
> - SCR entropy regularizationì´ text-dense ì˜ì—­ attention ì§‘ì¤‘ë„ í–¥ìƒ
> - 200-sample subset: Halluc 19.0% (SCRì´ ìˆ«ì hallucination 39â†’38ê±´ìœ¼ë¡œ ì¶”ê°€ ê°ì†Œ)
>
> Full encoder ft (337M params)ëŠ” catastrophic forgetting ìœ ë°œ:
> - ì •í™•ë„ 0.509 (-17.9%) â†’ pretrained ì‹œê° ëŠ¥ë ¥ ìƒì‹¤
> - Hallucination 23.0%ë¡œ ì†Œí­ ê°œì„ ë˜ì—ˆìœ¼ë‚˜ ì •í™•ë„ í•˜ë½ì´ ì‹¬ê°

### Hallucination ì„¸ë¶€ ë¶„ì„ (200-sample subset, seed=42)

| ë¶„ë¥˜ | Baseline | Full encoder ft | SFA-only (frozen) | SFA+ADAT (frozen) | SFA+ADAT+SCR (frozen) |
|------|----------|----------------|-------------------|-------------------|----------------------|
| ì •ë‹µ | 130 (65.0%) | 105 (52.5%) | 130 (65.0%) | 130 (65.0%) | **131 (65.5%)** |
| ìˆ«ì Hallucination | 51 (25.5%) | 46 (23.0%) | 39 (19.5%) | 39 (19.5%) | **38 (19.0%)** |
| ì˜¤ë‹µ (ê¸°íƒ€) | 19 (9.5%) | 49 (24.5%) | 31 (15.5%) | 31 (15.5%) | **31 (15.5%)** |

> SFA+ADAT+SCR: ìˆ«ì hallucination **51â†’38ê±´** (25.5% ê°ì†Œ), ì •ë‹µ 131ê±´ (+1)
> SCR entropy regularizationì´ text-dense íŒ¨ì¹˜ì˜ attention ì—”íŠ¸ë¡œí”¼ë¥¼ 4.082â†’4.075ë¡œ ê°ì†Œì‹œí‚´

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table 3: Training Strategy Ablation** â€” 4-way ë¹„êµ | `experiments/results/06_ablation_sfa_only/eval/summary.json` | âœ… ì™„ë£Œ |
| **Hallucination ì„¸ë¶€ ë¶„ì„** â€” 200-sample subset 4-way ë¹„êµ | `experiments/results/06_ablation_sfa_only/eval/hallucination.json` | âœ… ì™„ë£Œ |
| **SFA+ADAT í‰ê°€ ê²°ê³¼** | `experiments/results/07_sfa_adat/eval/summary.json` | âœ… ì™„ë£Œ |
| **SFA+ADAT Hallucination ë¶„ì„** | `experiments/results/07_sfa_adat/eval/hallucination.json` | âœ… ì™„ë£Œ |
| **SCR í‰ê°€ ê²°ê³¼** | `experiments/results/08_scr/eval/summary.json` | âœ… ì™„ë£Œ |
| **SCR Hallucination ë¶„ì„** | `experiments/results/08_scr/eval/hallucination.json` | âœ… ì™„ë£Œ |

---

## Slide 12. ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì „ì²´ êµ¬ì¡°

```
Phase 0: Baseline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ ChartQA Acc=0.620, Halluc=25.5%

Phase 1: ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ Figure 1, 2, 5 ìŠ¤í¬ë¦½íŠ¸ + Baseline ìƒì„±

Phase 2: SFA Fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â”œâ†’ P2-1: ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ âœ…
  â”œâ†’ P2-2: í•™ìŠµ ì‹¤í–‰ âœ… (3 epochs, loss 4.78â†’0.46)
  â”œâ†’ P2-3: ChartQA eval âœ… (Acc: 0.509, í•˜ë½ â†’ Catastrophic Forgetting)
  â”œâ†’ P2-4: Entropy ì¬ì¸¡ì • âœ… (Figure 5 ì¬ìƒì„± ì™„ë£Œ)
  â”œâ†’ P2-5: Hallucination ì¬ì¸¡ì • âœ… (23.0%, ì†Œí­ ê°œì„ )
  â”œâ†’ P2-6: Attention heatmap âœ… (Figure 3 ìƒì„± ì™„ë£Œ)
  â”œâ†’ P2-7: Structural bias ì‹œê°í™” âœ… (Figure 7 ìƒì„±)
  â””â†’ P2-8: Component ablation âœ… (SFA-only: Acc 0.6244, Halluc 20.2%)

Phase 3: ADAT êµ¬í˜„ + í†µí•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ Density-guided block assignment: Acc 0.6284, Halluc 20.0%

Phase 4: Full System (SCR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ Entropy regularization: Acc 0.6288, Halluc 20.0% (200-sample: 19.0%)

Phase 5: ë…¼ë¬¸ ì‘ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ eccv2016submission.tex + egbib.bib (31 references)

Phase 6: Multi-Benchmark í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â”œâ†’ DocVQA (ANLS): Baseline 0.536 â†’ SFA 0.529
  â”œâ†’ InfographicVQA (ANLS): Baseline 0.387 â†’ SFA 0.385
  â”œâ†’ DVQA (Exact Match): Baseline 0.410 â†’ SFA 0.408
  â””â†’ FigureQA (Exact Match): Baseline 0.956 â†’ SFA 0.956
```

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨ (ìœ„ í…ìŠ¤íŠ¸ êµ¬ì¡°ë„) | ë³¸ë¬¸ ë‚´ | âœ… ì™„ë£Œ |

---

## Slide 13. ê¸°ëŒ€ ê¸°ì—¬ì  ë° íƒ€ì„ë¼ì¸

### ê¸°ëŒ€ ê¸°ì—¬ì 

1. **Structure-Factorized Attention**: 0.002% íŒŒë¼ë¯¸í„° ì¶”ê°€ë¡œ ë¬¸ì„œ êµ¬ì¡° ì¸ì‹ ê°•í™”
2. **Adaptive Density-Aware Tokenization**: ë™ì¼ í† í° ìˆ˜ì—ì„œ ì •ë³´ëŸ‰ ê·¹ëŒ€í™”
3. **Hallucination ê°ì†Œ**: Attention entropy ê°ì†Œ + ìˆ«ì grounding ì•ˆì •í™”
4. **Architecture-Agnostic**: InternViT/SigLIP/CLIP ë“± ë‹¤ì–‘í•œ ViTì— ì ìš© ê°€ëŠ¥
5. **ë‹¤êµ­ì–´ ì¼ë°˜í™”**: í•œêµ­ì–´(AIDA, AIHUB) + ì˜ì–´(ChartQA, DocVQA) ë™ì‹œ ê²€ì¦

### íƒ€ì„ë¼ì¸

| ê¸°ê°„ | ì‘ì—… | ìƒíƒœ |
|------|------|------|
| 2/20 | Baseline ë¶„ì„ + Density Estimator + SFA ëª¨ë“ˆ | âœ… ì™„ë£Œ |
| 2/20 | SFA í†µí•© + Entropy/Hallucination ë¶„ì„ | âœ… ì™„ë£Œ |
| 2/23 | OOM í•´ê²° + 2-GPU í•™ìŠµ ì‹œì‘ | âœ… ì™„ë£Œ |
| 2/23~24 | SFA í•™ìŠµ ì™„ë£Œ + í›„ì† ë¶„ì„ | âœ… ì™„ë£Œ |
| 2/24~25 | ADAT êµ¬í˜„ + SFA+ADAT í†µí•© | âœ… ì™„ë£Œ |
| 2/25 | Full System (SCR) í•™ìŠµ + í‰ê°€ | âœ… ì™„ë£Œ |
| 2/25 | ë…¼ë¬¸ ì‘ì„± | âœ… ì™„ë£Œ |
| 2/25 | Multi-Benchmark í‰ê°€ (DocVQA, InfographicVQA, DVQA, FigureQA) | âœ… ì™„ë£Œ |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| íƒ€ì„ë¼ì¸ í‘œ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |

---

## ìë£Œ í˜„í™© ìš”ì•½

### âœ… ì™„ë£Œëœ Figure/Table ëª©ë¡

| # | ìë£Œëª… | íŒŒì¼ ê²½ë¡œ | ì‚¬ìš© ìŠ¬ë¼ì´ë“œ |
|---|--------|----------|-------------|
| 1 | Figure 1: Motivation (3-panel) | `experiments/figures/fig1_motivation/figure1_motivation.png` | Slide 1, 5 |
| 2 | Figure 2: Architecture Diagram | `experiments/figures/fig2_architecture.png` | Slide 4 |
| 3 | Figure 5: Entropy (Baseline vs SFA) | `experiments/figures/fig5_entropy/fig5_entropy.png` | Slide 3, 9 |
| 4 | Figure 7: Structural Bias (3ì¢…) | `experiments/figures/fig7_structural_bias/fig7{a,b,c}_*.png` | Slide 10 |
| 5 | ChartQA ìƒ˜í”Œ ì´ë¯¸ì§€ | `experiments/figures/sample_images/chartqa_sample.png` | Slide 1 |
| 6 | Density Map ì‹œê°í™” (20ì¥) | `experiments/results/01_density/visualizations/density_000~019.png` | Slide 5 |
| 7 | Token Efficiency Curve | `experiments/results/04_analysis/token_efficiency_curve.png` | Slide 5 |
| 8 | Baseline ì„±ëŠ¥ | `experiments/results/00_baseline/summary.json` | Slide 2, 8 |
| 9 | Baseline Hallucination ë¶„ì„ | `experiments/results/04_analysis/hallucination_analysis.json` | Slide 2 |
| 10 | Baseline Entropy ë¶„ì„ | `experiments/results/04_analysis/entropy_analysis.json` | Slide 3 |
| 11 | **SFA Eval ê²°ê³¼** | `experiments/results/05_sfa_eval/eval_results.json` | Slide 8 |
| 12 | **SFA Hallucination ë¶„ì„** | `experiments/results/05_sfa_eval/hallucination_sfa.json` | Slide 8 |
| 13 | **SFA Entropy ë¶„ì„** | `experiments/results/05_sfa_eval/entropy_analysis_sfa.json` | Slide 9 |
| 14 | í•™ìŠµ ë¡œê·¸ ë°ì´í„° | `experiments/results/03_sfa_train/train_log.json` | Slide 7 |
| 15 | **Loss Curve ê·¸ë˜í”„** | `experiments/figures/fig_loss_curve/loss_curve.png` | Slide 7 |
| 16 | **Loss Curve Detail (Epoch 2-3)** | `experiments/figures/fig_loss_curve/loss_curve_detail.png` | Slide 7 |
| 17 | **Figure 3: Attention Heatmap (Grid)** | `experiments/figures/fig3_attention_heatmap/fig3_attention_comparison.png` | Slide 10 |
| 18 | **Figure 3: Layer 23 Detail** | `experiments/figures/fig3_attention_heatmap/fig3_L23.png` | Slide 10 |
| 19 | **Table 3: Ablation (Strategy)** | `experiments/results/06_ablation_sfa_only/eval/summary.json` | Slide 11 |
| 20 | **Ablation Hallucination ë¶„ì„** | `experiments/results/06_ablation_sfa_only/eval/hallucination.json` | Slide 11 |
| 21 | **SFA+ADAT í•™ìŠµ ë¡œê·¸** | `experiments/results/07_sfa_adat/train_log.json` | Slide 5 |
| 22 | **SFA+ADAT í‰ê°€ ê²°ê³¼** | `experiments/results/07_sfa_adat/eval/summary.json` | Slide 5, 11 |
| 23 | **SFA+ADAT Hallucination ë¶„ì„** | `experiments/results/07_sfa_adat/eval/hallucination.json` | Slide 11 |
| 24 | **SFA+ADAT ChartQA ìƒì„¸ ê²°ê³¼** | `experiments/results/07_sfa_adat/eval/chartqa_eval.json` | Slide 11 |
| 25 | **SCR í•™ìŠµ ë¡œê·¸** | `experiments/results/08_scr/train_log.json` | Slide 11 |
| 26 | **SCR í‰ê°€ ê²°ê³¼** | `experiments/results/08_scr/eval/summary.json` | Slide 11 |
| 27 | **SCR Hallucination ë¶„ì„** | `experiments/results/08_scr/eval/hallucination.json` | Slide 11 |
| 28 | **SCR ChartQA ìƒì„¸ ê²°ê³¼** | `experiments/results/08_scr/eval/chartqa_eval.json` | Slide 11 |

| 29 | **Multi-Benchmark Summary** | `experiments/results/09_multi_benchmark/summary.json` | Slide 11 |
| 30 | **Baseline DocVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/baseline_docvqa.json` | Slide 11 |
| 31 | **SFA DocVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/sfa_docvqa.json` | Slide 11 |
| 32 | **Baseline InfographicVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/baseline_infographic_vqa.json` | Slide 11 |
| 33 | **SFA InfographicVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/sfa_infographic_vqa.json` | Slide 11 |
| 34 | **Baseline DVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/baseline_dvqa.json` | Slide 11 |
| 35 | **SFA DVQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/sfa_dvqa.json` | Slide 11 |
| 36 | **Baseline FigureQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/baseline_figureqa.json` | Slide 11 |
| 37 | **SFA FigureQA ê²°ê³¼** | `experiments/results/09_multi_benchmark/sfa_figureqa.json` | Slide 11 |

### âœ… ëª¨ë“  ì£¼ìš” ìë£Œ ì™„ë£Œ

> Phase 2~6 ì‹¤í—˜ì˜ ëª¨ë“  Figure/Tableì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ 37ê°œ ìë£Œ)

---

## Appendix: ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### SFA ìˆ˜ì‹ ìƒì„¸

32Ã—32 grid (1024 spatial tokens) + 1 CLS token = 1025 tokens

Precomputed buffers (í•™ìŠµ ì¤‘ ê³ ì •):
- `same_row[i,j]`: íŒ¨ì¹˜ i, jê°€ ê°™ì€ í–‰ì´ë©´ 1, ì•„ë‹ˆë©´ 0
- `same_col[i,j]`: ê°™ì€ ì—´ì´ë©´ 1
- `manhattan_dist[i,j]`: ì •ê·œí™”ëœ Manhattan ê±°ë¦¬

```python
# ì ìš© ìœ„ì¹˜: spatial tokensë§Œ (CLS ì œì™¸)
attn[:, :, 1:, 1:] += Ï†(structural_bias)
```

### í•™ìŠµ ë°ì´í„° ìƒì„¸

| Source | Samples | ìš©ë„ |
|--------|---------|------|
| ChartQA train (augmented) | 28K | SFA fine-tuning |
| ChartQA train (human) | 7.3K | SFA fine-tuning |
| mllm_ready V3 | 5.5M rows | ì¶”í›„ Phase 3-4 |
| Cauldron | 41GB | ì¶”í›„ Phase 3-4 |

### ëª¨ë¸ êµ¬ì¡°

```
InternVL3.5-8B
â”œâ”€â”€ vision_model (InternViT-300M)
â”‚   â”œâ”€â”€ embeddings (patch_embed + pos_embed)
â”‚   â””â”€â”€ encoder
â”‚       â””â”€â”€ layers Ã— 24
â”‚           â”œâ”€â”€ attn â†’ SFAAttention (êµì²´)
â”‚           â”‚   â”œâ”€â”€ qkv (1024â†’3072)
â”‚           â”‚   â”œâ”€â”€ proj (1024â†’1024)
â”‚           â”‚   â””â”€â”€ structural_bias (NEW, 304 params)
â”‚           â”œâ”€â”€ mlp (1024â†’4096â†’1024)
â”‚           â””â”€â”€ layer_scale Ã— 2
â”œâ”€â”€ mlp1 (projector: 4096â†’4096)
â””â”€â”€ language_model (InternLM2.5-7B, frozen)
```
