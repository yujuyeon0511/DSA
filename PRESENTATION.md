# Structure-Factorized Attention (SFA) for Document-Centric Multimodal LLMs

## ë°œí‘œìë£Œ

---

## Slide 1. ì—°êµ¬ ë°°ê²½ ë° ë¬¸ì œ ì •ì˜

### ê¸°ì¡´ Vision Encoderì˜ í•œê³„

í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ ë©€í‹°ëª¨ë‹¬ LLM (InternVL, Qwen-VL, LLaVA ë“±)ì€ **ViT ê¸°ë°˜ vision encoder**ë¥¼ ì‚¬ìš©.

**ë¬¸ì œì :**

- **Uniform Patch Tokenization**: ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë°€ì§‘ ì˜ì—­ê³¼ ë¹ˆ ê³µê°„ì— ë™ì¼í•œ í¬ê¸°ì˜ íŒ¨ì¹˜ ì ìš©
  - í…ìŠ¤íŠ¸ ì˜ì—­: ì •ë³´ê°€ ê³¼ë„í•˜ê²Œ ì••ì¶• â†’ ì‘ì€ ìˆ«ì/í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ë¥˜
  - ë¹ˆ ê³µê°„: ë¶ˆí•„ìš”í•œ í† í° ë‚­ë¹„

- **Layout Inductive Bias ë¶€ì¬**: ViTì˜ positional encodingì€ ë¬¸ì„œì˜ í–‰/ì—´/ë¸”ë¡ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨

- **ìˆ«ì Hallucination**: ì°¨íŠ¸/í‘œì—ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìˆ«ìë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ì‹¬ê°

### í•µì‹¬ ì—°êµ¬ ì§ˆë¬¸

> ë¬¸ì„œ ì´ë¯¸ì§€ì˜ **êµ¬ì¡°ì  íŠ¹ì„±**ì„ vision encoderì˜ attentionì— ì§ì ‘ ì£¼ì…í•˜ë©´,
> groundingì´ ì•ˆì •í™”ë˜ê³  hallucinationì´ ì¤„ì–´ë“œëŠ”ê°€?

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 1: Motivation** â€” (a) Uniform grid (b) Density heatmap (c) Adaptive patching | `experiments/figures/fig1_motivation/figure1_motivation.png` | âœ… ì™„ë£Œ |
| **ChartQA ìƒ˜í”Œ ì´ë¯¸ì§€** â€” ë¬¸ì œ ìƒí™© ì˜ˆì‹œìš© ì°¨íŠ¸ ì´ë¯¸ì§€ | `experiments/figures/sample_images/chartqa_sample.png` | âœ… ì™„ë£Œ |

---

## Slide 2. Baseline ë¶„ì„ ê²°ê³¼

### ì‹¤í—˜ í™˜ê²½

| í•­ëª© | ê°’ |
|------|-----|
| Base Model | InternVL3.5-8B |
| Vision Encoder | InternViT-300M-448px (24 layers) |
| LLM | InternLM2.5-7B-Chat |
| GPU | NVIDIA A100-PCIE-40GB Ã— 2 |
| í‰ê°€ ë²¤ì¹˜ë§ˆí¬ | ChartQA (Relaxed Accuracy) |

### Baseline ì„±ëŠ¥

| Metric | Score |
|--------|-------|
| **ChartQA Relaxed Accuracy** | **0.620** |

### Hallucination ë¶„ì„ (200 samples)

| ë¶„ë¥˜ | ìˆ˜ | ë¹„ìœ¨ |
|------|-----|------|
| ì •ë‹µ | 130 | 65.0% |
| **ìˆ«ì Hallucination** | **51** | **25.5%** |
| ì˜¤ë‹µ (ê¸°íƒ€) | 19 | 9.5% |

> **ì˜¤ë¥˜ì˜ 73%ê°€ ìˆ«ì hallucination** â€” ì´ë¯¸ì§€ì— ì—†ëŠ” ìˆ«ìë¥¼ ìƒì„±

### ì˜¤ë‹µ ì˜ˆì‹œ

| Question | GT | Prediction | ì˜¤ë¥˜ ìœ í˜• |
|----------|-----|-----------|-----------|
| "What was the largest dark red bar value?" | 0.08 | **26** | ìˆ«ì hallucination |
| "What is the difference between highest and lowest?" | 54 | **99** | ìˆ«ì hallucination |
| "What's the value for the rightmost bar?" | 0.57 | **10.04** | ìë¦¿ìˆ˜ ì˜¤ë¥˜ |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table: Baseline ì„±ëŠ¥** â€” ChartQA Relaxed Accuracy = 0.620 | `experiments/results/00_baseline/summary.json` | âœ… ì™„ë£Œ |
| **Table: Hallucination ë¶„ì„** â€” 200 samples ë¶„ë¥˜ (ì •ë‹µ/halluc/ì˜¤ë‹µ) | `experiments/results/04_analysis/hallucination_analysis.json` | âœ… ì™„ë£Œ |
| **Table: ì˜¤ë‹µ ì˜ˆì‹œ** â€” hallucination_analysis.jsonì—ì„œ ëŒ€í‘œ ì˜¤ë‹µ 3ê±´ ì¶”ì¶œ | `experiments/results/04_analysis/hallucination_analysis.json` | âœ… ì™„ë£Œ |

---

## Slide 3. Attention Entropy ë¶„ì„ (Baseline)

### Attention Entropy ë¶„ì„

| ì˜ì—­ | Entropy | ì°¨ì´ |
|------|---------|------|
| Text-dense region | 4.3322 | - |
| Sparse region | 4.4377 | - |
| **ë¹„ìœ¨** | **0.98x** | â‰ˆ ë™ì¼ |

> Text-dense/sparse ê°„ attention entropy ì°¨ì´ê°€ ê±°ì˜ ì—†ìŒ
> â†’ **Vision encoderê°€ ë¬¸ì„œ êµ¬ì¡°ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•¨** â†’ SFA í•„ìš”ì„± ì…ì¦

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 5: Entropy Analysis (Baseline)** â€” Violin plot + Layer-wise line plot | `experiments/figures/fig5_entropy/fig5_entropy.png` | âœ… ì™„ë£Œ |
| **Table: Entropy í†µê³„** â€” text-dense vs sparse ì˜ì—­ entropy ë¹„êµ | `experiments/results/04_analysis/entropy_analysis.json` | âœ… ì™„ë£Œ |
| **Figure 5: Entropy Analysis (Baseline vs SFA)** â€” í•™ìŠµ í›„ ë¹„êµ ì¶”ê°€ | â³ SFA í•™ìŠµ ì™„ë£Œ í›„ ì¬ìƒì„± | â³ ì˜ˆì • (P2-4) |

---

## Slide 4. ì œì•ˆ ë°©ë²•: Structure-Factorized Attention (SFA)

### í•µì‹¬ ì•„ì´ë””ì–´

ê¸°ì¡´ ViT self-attentionì— **ë¬¸ì„œ êµ¬ì¡° bias**ë¥¼ ì¶”ê°€:

```
ê¸°ì¡´:  S_ij = (Q_i Â· K_j^T) / âˆšd

ì œì•ˆ:  S_ij = (Q_i Â· K_j^T) / âˆšd  +  Ï†(s_i, s_j)
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               Content Attention    Structural Bias
```

### Structural Bias Ï†(s_i, s_j) êµ¬ì„±

```
Ï† = w_row Â· ğŸ™[row_i = row_j]           â† ê°™ì€ í–‰ íŒ¨ì¹˜ ê°„ ê°•í™”
  + w_col Â· ğŸ™[col_i = col_j]           â† ê°™ì€ ì—´ íŒ¨ì¹˜ ê°„ ê°•í™”
  + w_dist Â· (-manhattan(i,j))          â† ê°€ê¹Œìš´ íŒ¨ì¹˜ ê°„ ê°•í™”
  + block_embed(b_i)^T Â· block_embed(b_j)  â† ê°™ì€ ë¸”ë¡ ê°„ ê°•í™”
```

### íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±

| Component | Parameters | Overhead |
|-----------|-----------|----------|
| w_row (per head) | 16 | - |
| w_col (per head) | 16 | - |
| w_dist (per head) | 16 | - |
| block_embed (16 blocks Ã— 16 heads) | 256 | - |
| **Layerë‹¹ í•©ê³„** | **304** | **0.007%** |
| **ì „ì²´ (24 layers)** | **7,296** | **0.002%** |

> ì „ì²´ ëª¨ë¸ ëŒ€ë¹„ **0.002%ì˜ íŒŒë¼ë¯¸í„°**ë§Œ ì¶”ê°€í•˜ë©´ì„œ êµ¬ì¡°ì  inductive bias ì œê³µ

### ì ìš© ë°©ì‹

- InternViTì˜ **24ê°œ attention layer ëª¨ë‘**ì— SFA ì ìš©
- CLS í† í°ì—ëŠ” structural bias ë¯¸ì ìš© (spatial tokensë§Œ)
- Pretrained QKV weights ìœ ì§€, structural biasë§Œ small init (std=0.02)

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 2: Architecture Diagram** â€” ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë„ | `experiments/figures/fig2_architecture.png` | âœ… ì™„ë£Œ |
| **Table: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±** â€” SFA ì¶”ê°€ íŒŒë¼ë¯¸í„° ë¶„ì„ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |

---

## Slide 5. Adaptive Density-Aware Tokenization (ADAT)

### ë™ê¸°

ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ:
- í…ìŠ¤íŠ¸ ë°€ì§‘ ì˜ì—­ â†’ ë” ì‘ì€ íŒ¨ì¹˜ í•„ìš” (ë†’ì€ í•´ìƒë„)
- ë¹ˆ ê³µê°„ â†’ í° íŒ¨ì¹˜ë¡œ ì¶©ë¶„ (í† í° ì ˆì•½)

### Text Density Estimator

| í•­ëª© | ê°’ |
|------|-----|
| Architecture | 6-layer CNN |
| Parameters | **186K** (ê²½ëŸ‰) |
| Input | 448Ã—448 document image |
| Output | 28Ã—28 density heatmap D(x,y) âˆˆ [0,1] |
| Training | Self-supervised (pseudo labels) |
| Val Loss | **0.001728** |

### Pseudo Label ìƒì„±

```
Document Image â†’ Canny Edge Detection â†’ Adaptive Threshold â†’ Gaussian Blur â†’ Density Map
```

ë³„ë„ annotation ì—†ì´ ì´ë¯¸ì§€ ìì²´ì—ì„œ í…ìŠ¤íŠ¸ ë°€ë„ ì¶”ì • ê°€ëŠ¥

### Adaptive Patch ì „ëµ (ê³„íš)

| Density | Patch Size | í† í° ìˆ˜ |
|---------|-----------|---------|
| D > 0.7 (high) | 8Ã—8 | ë§ìŒ (ì„¸ë°€) |
| 0.3 < D â‰¤ 0.7 (medium) | 14Ã—14 | í‘œì¤€ |
| D â‰¤ 0.3 (low) | 32Ã—32 | ì ìŒ (íš¨ìœ¨) |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 1: Motivation (b) Density Heatmap** â€” ë°€ë„ ì¶”ì • ê²°ê³¼ | `experiments/figures/fig1_motivation/figure1_motivation.png` (panel b) | âœ… ì™„ë£Œ |
| **Density Map ì‹œê°í™”** â€” 20ê°œ ChartQA ì´ë¯¸ì§€ì˜ density estimation ê²°ê³¼ | `experiments/results/01_density/visualizations/density_000~019.png` | âœ… ì™„ë£Œ |
| **Token Efficiency Curve** â€” ë°€ë„ ê¸°ë°˜ í† í° ì ˆì•½ íš¨ê³¼ ê·¸ë˜í”„ | `experiments/results/04_analysis/token_efficiency_curve.png` | âœ… ì™„ë£Œ |

---

## Slide 6. í•™ìŠµ ì „ëµ ë° OOM í•´ê²°

### í•™ìŠµ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trainable (4.0%)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ InternViT-300M   â”‚  â”‚ MLP Projector    â”‚     â”‚
â”‚  â”‚ + SFA (24 layers)â”‚â†’â”‚ (4096â†’4096)      â”‚     â”‚
â”‚  â”‚ GPU 0: 8.1 GB    â”‚  â”‚                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                 â”‚               â”‚
â”‚  Frozen (96.0%)                 â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ InternLM2.5-7B-Chat (bf16)              â”‚   â”‚
â”‚  â”‚ GPU 1: 15.3 GB                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### OOM ë¬¸ì œ ë° í•´ê²°

**ë¬¸ì œ**: InternVL3.5-8B (17GB bf16)ì„ A100-40GBì—ì„œ fine-tuning ì‹œ OOM

**í•´ê²° (2-GPU Model Parallel):**

| ì „ëµ | ë‚´ìš© | íš¨ê³¼ |
|------|------|------|
| Vision â†’ GPU 0 | Vision encoder + projector (trainable) | 8.1 GB |
| LLM â†’ GPU 1 | LLM (frozen, bf16 full) | 15.3 GB |
| Gradient Checkpointing | Vision encoder í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê° | -50% activation |
| batch_size=4 | GPU ì—¬ìœ  í™œìš© | ì²˜ë¦¬ëŸ‰ 4ë°°â†‘ |

**ì´ì „ ì‹œë„ (ì‹¤íŒ¨):**
- Single GPU full bf16 â†’ OOM (17GB + optimizer + activations > 40GB)
- DDP 2-GPU â†’ ê° rankë§ˆë‹¤ full model ë³µì‚¬ â†’ OOM

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **í•™ìŠµ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨** â€” 2-GPU ë°°ì¹˜ (ìœ„ ASCII ë‹¤ì´ì–´ê·¸ë¨ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ ë‹¤ì´ì–´ê·¸ë¨ | âœ… ì™„ë£Œ |
| **Table: OOM í•´ê²° ì „ëµ** â€” GPU ë¶„í•  + checkpointing íš¨ê³¼ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **Table: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** â€” ì‹¤ì¸¡ ê°’ | `experiments/results/03_sfa_train/train.log` (G0:8.1GB G1:15.3GB) | âœ… ì™„ë£Œ |

---

## Slide 7. SFA í•™ìŠµ ì§„í–‰ ìƒí™© ë° Loss ì¶”ì´

### í•™ìŠµ ì„¤ì •

| í•­ëª© | ê°’ |
|------|-----|
| Data | ChartQA train (28,299 samples) |
| Effective batch size | 4 Ã— 8 (grad_accum) = **32** |
| Epochs | 3 |
| Optimizer | AdamW (lr=2e-5, cosine schedule) |
| Total optimizer steps | 2,652 |

### Loss ì¶”ì´

```
Epoch 1:
  step   80 | loss: 4.7813  â† ì´ˆê¸°
  step  400 | loss: 4.3514
  step 2000 | loss: 1.3860
  step 7074 | loss: 1.1015  â† Epoch 1 ì™„ë£Œ

Epoch 2:
  step   80 | loss: 0.6696
  step 2000 | loss: 0.5361
  step 7074 | loss: 0.5088  â† Epoch 2 ì™„ë£Œ

Epoch 3:
  step   80 | loss: 0.5311
  step  240 | loss: 0.4736  â† ì§„í–‰ ì¤‘
```

> **Loss: 4.78 â†’ 0.47 (ì•½ 90% ê°ì†Œ)** â€” í•™ìŠµ ì •ìƒ ìˆ˜ë ´ ì¤‘

### GPU í™œìš©ë¥ 

| GPU | ì—­í•  | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì—¬ìœ  |
|-----|------|-----------|------|
| GPU 0 | Vision (trainable) | 8.1 GB | 32.9 GB (80%) |
| GPU 1 | LLM (frozen) | 15.3 GB | 25.7 GB (63%) |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Loss Curve ê·¸ë˜í”„** â€” train.logì—ì„œ ì¶”ì¶œí•˜ì—¬ ìƒì„± | `experiments/results/03_sfa_train/train.log` â†’ ê·¸ë˜í”„ ìƒì„± í•„ìš” | â³ ì˜ˆì • (í•™ìŠµ ì™„ë£Œ í›„) |
| **Table: í•™ìŠµ ì„¤ì •** â€” hyperparameter ìš”ì•½ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **Table: GPU í™œìš©ë¥ ** â€” ì‹¤ì¸¡ GPU ë©”ëª¨ë¦¬ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |
| **í•™ìŠµ ë¡œê·¸ ë°ì´í„°** â€” stepë³„ loss/lr ê¸°ë¡ | `experiments/results/03_sfa_train/train_log.json` | âœ… ì™„ë£Œ |

---

## Slide 8. SFA ì„±ëŠ¥ í‰ê°€ ê²°ê³¼

### Table 1: ChartQA ì„±ëŠ¥ ë¹„êµ (Main Result)

| Model | ChartQA Relaxed Acc | ë³€í™” |
|-------|-------------------|------|
| InternVL3.5-8B (Baseline) | 0.620 | - |
| **+ SFA (Ours)** | **â³ ì¸¡ì • ì˜ˆì •** | **â³** |

### Table 2: Hallucination ë¹„êµ (200 samples)

| ë¶„ë¥˜ | Baseline | + SFA |
|------|----------|-------|
| ì •ë‹µ | 130 (65.0%) | â³ |
| ìˆ«ì Hallucination | 51 (25.5%) | â³ |
| ì˜¤ë‹µ (ê¸°íƒ€) | 19 (9.5%) | â³ |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table 1: ChartQA ì„±ëŠ¥ ë¹„êµ** â€” Baseline vs +SFA | `experiments/results/00_baseline/summary.json` + SFA eval ê²°ê³¼ | â³ ì˜ˆì • (P2-3) |
| **Table 2: Hallucination ë¹„êµ** â€” Baseline vs +SFA hallucination ë¶„ë¥˜ | `experiments/results/04_analysis/hallucination_analysis.json` + SFA ì¬ì¸¡ì • | â³ ì˜ˆì • (P2-5) |

---

## Slide 9. Entropy ë¶„ì„ â€” Baseline vs SFA

### Attention Entropy ë¹„êµ

| ì˜ì—­ | Baseline | + SFA | ë³€í™” |
|------|----------|-------|------|
| Text-dense region | 4.3322 | â³ | â³ |
| Sparse region | 4.4377 | â³ | â³ |
| Dense/Sparse ë¹„ìœ¨ | 0.98x | â³ | â³ |

> SFA ì ìš© í›„ text-dense ì˜ì—­ì˜ entropyê°€ ë‚®ì•„ì§€ë©´ â†’ êµ¬ì¡° ì¸ì‹ ê°•í™” ì…ì¦

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 5: Entropy Analysis (Baseline vs SFA)** â€” Violin + Layer-wise ë¹„êµ | `experiments/figures/fig5_entropy/fig5_entropy.png` (Baseline ì™„ë£Œ, SFA í›„ ì¬ìƒì„±) | â³ ì˜ˆì • (P2-4) |
| **Table: Entropy ë¹„êµ** â€” Baseline vs SFA entropy í†µê³„ | `experiments/results/04_analysis/entropy_analysis.json` + SFA ì¬ì¸¡ì • | â³ ì˜ˆì • (P2-4) |

---

## Slide 10. Attention Heatmap ì‹œê°í™”

### Attention Map ë¹„êµ â€” Baseline vs SFA

ë™ì¼ ì°¨íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ attention heatmap ë¹„êµ:
- **Baseline**: ì „ì²´ì— ê³ ë¥´ê²Œ ë¶„ì‚° (êµ¬ì¡° ë¬´ì‹œ)
- **+SFA**: í…ìŠ¤íŠ¸/ìˆ«ì ì˜ì—­ì— ì§‘ì¤‘ (êµ¬ì¡° ì¸ì‹)

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Figure 3: Attention Heatmap (Baseline vs SFA)** â€” ë™ì¼ ì´ë¯¸ì§€ 2-panel ë¹„êµ | â³ í•™ìŠµ ì™„ë£Œ í›„ ìƒì„± | â³ ì˜ˆì • (P2-6) |
| **Figure 7: Structural Bias ì‹œê°í™”** â€” í•™ìŠµëœ w_row, w_col, w_dist ê°’ ë¶„í¬ | â³ í•™ìŠµ ì™„ë£Œ í›„ ìƒì„± | â³ ì˜ˆì • (P2-7) |

---

## Slide 11. Ablation Study

### Table 3: Component Ablation

| Configuration | ChartQA Acc | Entropy Ratio | Halluc Rate |
|--------------|-------------|---------------|-------------|
| Baseline (no SFA) | 0.620 | 0.98x | 25.5% |
| + row/col only | â³ | â³ | â³ |
| + row/col + dist | â³ | â³ | â³ |
| + full SFA (all components) | â³ | â³ | â³ |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| **Table 3: Component Ablation** â€” SFA ê° êµ¬ì„±ìš”ì†Œì˜ ê¸°ì—¬ë„ ë¶„ì„ | â³ í•™ìŠµ ì™„ë£Œ í›„ ablation ì‹¤í—˜ í•„ìš” | â³ ì˜ˆì • (P2-8) |

---

## Slide 12. ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì „ì²´ êµ¬ì¡°

```
Phase 0: Baseline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ ChartQA Acc=0.620, Halluc=25.5%

Phase 1: ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… ì™„ë£Œ
  â””â†’ Figure 1, 2, 5 ìŠ¤í¬ë¦½íŠ¸ + Baseline ìƒì„±

Phase 2: SFA Fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ”„ ì§„í–‰ ì¤‘
  â”œâ†’ P2-1: ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„ âœ…
  â”œâ†’ P2-2: í•™ìŠµ ì‹¤í–‰   ğŸ”„ (Epoch 3/3)
  â”œâ†’ P2-3: ChartQA eval â†’ Table 1 "+SFA"
  â”œâ†’ P2-4: Entropy ì¬ì¸¡ì • â†’ Figure 5 ì™„ì„±
  â”œâ†’ P2-5: Hallucination ì¬ì¸¡ì • â†’ Table 2
  â”œâ†’ P2-6: Attention heatmap â†’ Figure 3 ì™„ì„±
  â”œâ†’ P2-7: Structural bias ì‹œê°í™” â†’ Figure 7
  â””â†’ P2-8: Component ablation â†’ Table 3

Phase 3: ADAT êµ¬í˜„ + í†µí•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¬œ ì˜ˆì •
  â””â†’ Adaptive tokenization + Token efficiency ì‹¤ì¸¡

Phase 4: Full System (SCR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¬œ ì˜ˆì •
  â””â†’ Entropy/Grounding/Stability loss â†’ Hallucination ê°ì†Œ

Phase 5: Cross-Architecture + ë…¼ë¬¸ â”€â”€â”€â”€â”€â”€ â¬œ ì˜ˆì •
  â”œâ†’ SFA â†’ Qwen2.5-VL (SigLIP+Qwen) ì ìš©
  â”œâ†’ SFA â†’ LLaVA-OV (CLIP+LLaMA) ì ìš©
  â””â†’ ë…¼ë¬¸ ì‘ì„±
```

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨ (ìœ„ í…ìŠ¤íŠ¸ êµ¬ì¡°ë„) | ë³¸ë¬¸ ë‚´ | âœ… ì™„ë£Œ |

---

## Slide 13. ê¸°ëŒ€ ê¸°ì—¬ì  ë° íƒ€ì„ë¼ì¸

### ê¸°ëŒ€ ê¸°ì—¬ì 

1. **Structure-Factorized Attention**: 0.002% íŒŒë¼ë¯¸í„° ì¶”ê°€ë¡œ ë¬¸ì„œ êµ¬ì¡° ì¸ì‹ ê°•í™”
2. **Adaptive Density-Aware Tokenization**: ë™ì¼ í† í° ìˆ˜ì—ì„œ ì •ë³´ëŸ‰ ê·¹ëŒ€í™”
3. **Hallucination ê°ì†Œ**: Attention entropy ê°ì†Œ + ìˆ«ì grounding ì•ˆì •í™”
4. **Architecture-Agnostic**: InternViT/SigLIP/CLIP ë“± ë‹¤ì–‘í•œ ViTì— ì ìš© ê°€ëŠ¥
5. **ë‹¤êµ­ì–´ ì¼ë°˜í™”**: í•œêµ­ì–´(AIDA, AIHUB) + ì˜ì–´(ChartQA, DocVQA) ë™ì‹œ ê²€ì¦

### íƒ€ì„ë¼ì¸

| ê¸°ê°„ | ì‘ì—… | ìƒíƒœ |
|------|------|------|
| 2/20 | Baseline ë¶„ì„ + Density Estimator + SFA ëª¨ë“ˆ | âœ… ì™„ë£Œ |
| 2/20 | SFA í†µí•© + Entropy/Hallucination ë¶„ì„ | âœ… ì™„ë£Œ |
| 2/23 | OOM í•´ê²° + 2-GPU í•™ìŠµ ì‹œì‘ | âœ… ì™„ë£Œ |
| 2/23~24 | SFA í•™ìŠµ ì™„ë£Œ + í›„ì† ë¶„ì„ | ğŸ”„ ì§„í–‰ ì¤‘ |
| 2/24~25 | ADAT êµ¬í˜„ + SFA+ADAT í†µí•© | â¬œ ì˜ˆì • |
| 2/25~26 | Full System (SCR) í•™ìŠµ | â¬œ ì˜ˆì • |
| 2/27~28 | Cross-Architecture ì‹¤í—˜ | â¬œ ì˜ˆì • |
| 3/1~ | ë…¼ë¬¸ ì‘ì„± | â¬œ ì˜ˆì • |

#### ğŸ“ ì´ ìŠ¬ë¼ì´ë“œì— í¬í•¨í•  ìë£Œ

| ìë£Œ | íŒŒì¼ ê²½ë¡œ | ìƒíƒœ |
|------|----------|------|
| íƒ€ì„ë¼ì¸ í‘œ (ìœ„ í‘œ ì‚¬ìš©) | ë³¸ë¬¸ ë‚´ í‘œ | âœ… ì™„ë£Œ |

---

## ìë£Œ í˜„í™© ìš”ì•½

### âœ… ì™„ë£Œëœ Figure/Table ëª©ë¡

| # | ìë£Œëª… | íŒŒì¼ ê²½ë¡œ | ì‚¬ìš© ìŠ¬ë¼ì´ë“œ |
|---|--------|----------|-------------|
| 1 | Figure 1: Motivation (3-panel) | `experiments/figures/fig1_motivation/figure1_motivation.png` | Slide 1, 5 |
| 2 | Figure 2: Architecture Diagram | `experiments/figures/fig2_architecture.png` | Slide 4 |
| 3 | Figure 5: Entropy (Baseline) | `experiments/figures/fig5_entropy/fig5_entropy.png` | Slide 3 |
| 4 | ChartQA ìƒ˜í”Œ ì´ë¯¸ì§€ | `experiments/figures/sample_images/chartqa_sample.png` | Slide 1 |
| 5 | Density Map ì‹œê°í™” (20ì¥) | `experiments/results/01_density/visualizations/density_000~019.png` | Slide 5 |
| 6 | Token Efficiency Curve | `experiments/results/04_analysis/token_efficiency_curve.png` | Slide 5 |
| 7 | Baseline ì„±ëŠ¥ (summary.json) | `experiments/results/00_baseline/summary.json` | Slide 2, 8 |
| 8 | Hallucination ë¶„ì„ ë°ì´í„° | `experiments/results/04_analysis/hallucination_analysis.json` | Slide 2, 8 |
| 9 | Entropy ë¶„ì„ ë°ì´í„° | `experiments/results/04_analysis/entropy_analysis.json` | Slide 3, 9 |
| 10 | í•™ìŠµ ë¡œê·¸ ë°ì´í„° | `experiments/results/03_sfa_train/train_log.json` | Slide 7 |

### â³ SFA í•™ìŠµ ì™„ë£Œ í›„ ìƒì„± ì˜ˆì •

| # | ìë£Œëª… | ìƒì„± Phase | ì‚¬ìš© ìŠ¬ë¼ì´ë“œ |
|---|--------|-----------|-------------|
| 1 | Table 1: ChartQA ì„±ëŠ¥ ë¹„êµ (Baseline vs +SFA) | P2-3 | Slide 8 |
| 2 | Figure 5: Entropy ë¹„êµ (Baseline vs SFA, ì¬ìƒì„±) | P2-4 | Slide 9 |
| 3 | Table 2: Hallucination ë¹„êµ (Baseline vs +SFA) | P2-5 | Slide 8 |
| 4 | Figure 3: Attention Heatmap (Baseline vs SFA) | P2-6 | Slide 10 |
| 5 | Figure 7: Structural Bias ì‹œê°í™” | P2-7 | Slide 10 |
| 6 | Table 3: Component Ablation | P2-8 | Slide 11 |
| 7 | Loss Curve ê·¸ë˜í”„ | í•™ìŠµ ì™„ë£Œ í›„ | Slide 7 |

---

## Appendix: ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### SFA ìˆ˜ì‹ ìƒì„¸

32Ã—32 grid (1024 spatial tokens) + 1 CLS token = 1025 tokens

Precomputed buffers (í•™ìŠµ ì¤‘ ê³ ì •):
- `same_row[i,j]`: íŒ¨ì¹˜ i, jê°€ ê°™ì€ í–‰ì´ë©´ 1, ì•„ë‹ˆë©´ 0
- `same_col[i,j]`: ê°™ì€ ì—´ì´ë©´ 1
- `manhattan_dist[i,j]`: ì •ê·œí™”ëœ Manhattan ê±°ë¦¬

```python
# ì ìš© ìœ„ì¹˜: spatial tokensë§Œ (CLS ì œì™¸)
attn[:, :, 1:, 1:] += Ï†(structural_bias)
```

### í•™ìŠµ ë°ì´í„° ìƒì„¸

| Source | Samples | ìš©ë„ |
|--------|---------|------|
| ChartQA train (augmented) | 28K | SFA fine-tuning |
| ChartQA train (human) | 7.3K | SFA fine-tuning |
| mllm_ready V3 | 5.5M rows | ì¶”í›„ Phase 3-4 |
| Cauldron | 41GB | ì¶”í›„ Phase 3-4 |

### ëª¨ë¸ êµ¬ì¡°

```
InternVL3.5-8B
â”œâ”€â”€ vision_model (InternViT-300M)
â”‚   â”œâ”€â”€ embeddings (patch_embed + pos_embed)
â”‚   â””â”€â”€ encoder
â”‚       â””â”€â”€ layers Ã— 24
â”‚           â”œâ”€â”€ attn â†’ SFAAttention (êµì²´)
â”‚           â”‚   â”œâ”€â”€ qkv (1024â†’3072)
â”‚           â”‚   â”œâ”€â”€ proj (1024â†’1024)
â”‚           â”‚   â””â”€â”€ structural_bias (NEW, 304 params)
â”‚           â”œâ”€â”€ mlp (1024â†’4096â†’1024)
â”‚           â””â”€â”€ layer_scale Ã— 2
â”œâ”€â”€ mlp1 (projector: 4096â†’4096)
â””â”€â”€ language_model (InternLM2.5-7B, frozen)
```
