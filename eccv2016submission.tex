% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{color}
\usepackage{bm}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCV16SubNumber{***}

\title{Structure-Factorized Attention for\\Document-Centric Multimodal Models}

\titlerunning{Structure-Factorized Attention}

\authorrunning{Anonymous}

\author{Anonymous ECCV submission}
\institute{Paper ID \ECCV16SubNumber}

\maketitle

%======================================================================
\begin{abstract}
%======================================================================

Multimodal large language models (MLLMs) achieve strong performance on document-centric tasks, yet their vision encoders rely on uniform patch tokenization and content-only self-attention, overlooking the structured nature of documents.
This mismatch leads to unstable visual grounding and numeric hallucination, particularly in chart and table understanding.

We propose \textbf{Structure-Factorized Attention (SFA)}, which decomposes self-attention into content-based and structure-based components by introducing a learnable structural bias directly into the attention kernel.
SFA encodes row, column, and block-level spatial relationships through per-head bias terms, enabling layout-aware token interactions with only \textbf{7,296 additional parameters} (0.002\% of the base model).
We further introduce \textbf{density-guided block assignment}, where a lightweight text density estimator provides semantic block structure to SFA, and \textbf{attention entropy regularization} to sharpen attention on text-dense regions.

Built upon InternVL3.5-8B with a frozen backbone strategy, SFA reduces numeric hallucination from 25.5\% to 20.0\% on ChartQA while improving accuracy from 62.0\% to 62.9\%.
Cross-benchmark evaluation on DocVQA, InfographicVQA, DVQA, and FigureQA confirms that the structural bias does not degrade general document understanding, demonstrating that minimal structural inductive bias at the encoder level can meaningfully improve document grounding without sacrificing generality.

\end{abstract}

\keywords{Multimodal Learning \and Document Understanding \and Vision Transformer \and Structural Inductive Bias \and Hallucination Reduction}


%======================================================================
\section{Introduction}
%======================================================================

Multimodal large language models (MLLMs) have achieved remarkable progress in visual reasoning and document understanding~\cite{chen2024internvl,bai2023qwen,liu2023llava}.
Recent systems combine large vision encoders with powerful language models to answer questions about charts, parse tables, and extract textual information from complex layouts.
Despite this progress, a core architectural assumption remains largely unchanged: most MLLMs adopt vision encoders originally designed for natural image recognition~\cite{dosovitskiy2021image,radford2021learning}.

Documents differ fundamentally from natural images.
They exhibit structured layouts, including aligned rows and columns, hierarchical blocks, and semantically distinct regions such as titles, legends, and data areas.
Moreover, information density is highly non-uniform, with small text and numeric regions coexisting alongside large blank margins.
Standard Vision Transformers (ViTs), however, partition images into uniform patches and compute attention purely based on content similarity.
This mismatch between document structure and encoder design leads to two critical problems: (1) \textit{inefficient attention allocation}, where tokens in sparse regions receive the same computational weight as information-rich regions, and (2) \textit{unstable visual grounding}, where the model generates plausible but incorrect numbers---a phenomenon known as \textit{numeric hallucination}.

Prior work primarily addresses these issues through scaling model size~\cite{chen2024far}, increasing training data, or improving multimodal alignment objectives~\cite{li2023blip2,alayrac2022flamingo}.
While effective, these approaches do not directly incorporate document-specific inductive bias into the vision encoder.
Layout information is typically injected via additive positional embeddings~\cite{xu2020layoutlm,xu2021layoutlmv2} or external OCR pipelines~\cite{kim2022donut}, which do not modify the attention mechanism itself.

In this work, we revisit the design of the vision encoder for document-centric multimodal learning.
We propose \textbf{Structure-Factorized Attention (SFA)}, which reformulates self-attention by decomposing attention scores into content and structural components.
Row alignment, column alignment, spatial distance, and block membership are introduced directly into the attention kernel through learnable per-head bias terms.
This encourages coherent interactions within structured document regions while preserving the pretrained content-similarity modeling.

A key design principle of SFA is \textit{extreme parameter efficiency}: the structural bias adds only 304 parameters per attention layer (7,296 total across 24 layers), representing 0.002\% of the base model.
Combined with a frozen backbone strategy---where only SFA parameters are trained while the entire vision encoder, projector, and language model remain frozen---this approach avoids catastrophic forgetting while injecting document-aware structure into the attention mechanism.

Our contributions are as follows:
\begin{itemize}
\item We identify the structural inductive bias gap in document-oriented MLLMs and propose Structure-Factorized Attention, which injects row, column, and block relationships directly into the attention kernel.
\item We introduce density-guided block assignment using a lightweight CNN estimator (186K parameters), which provides adaptive semantic grouping to SFA without modifying the token count.
\item We design an attention entropy regularization objective that sharpens attention on text-dense regions, further reducing hallucination.
\item We demonstrate that with only 7,296 trainable parameters, SFA reduces numeric hallucination by 25.5\% (from 51 to 38 cases per 200 samples) on ChartQA while improving accuracy. Mixed-benchmark training on five diverse datasets shows that SFA generalizes across document types, closing the cross-benchmark gap to within $\pm 0.2$\%p of the baseline.
\end{itemize}


%======================================================================
\section{Related Work}
%======================================================================

\subsection{Multimodal Large Language Models}

Recent MLLMs integrate vision encoders with large language models to enable unified visual-text reasoning.
CLIP~\cite{radford2021learning} and SigLIP~\cite{zhai2023sigmoid} provide pretrained visual representations, while architectures such as BLIP-2~\cite{li2023blip2}, Flamingo~\cite{alayrac2022flamingo}, LLaVA~\cite{liu2023llava}, Qwen-VL~\cite{bai2023qwen}, and InternVL~\cite{chen2024internvl} demonstrate strong performance across visual question answering, document understanding, and chart reasoning.
InternVL3.5~\cite{chen2024far} combines InternViT-300M with InternLM2.5-7B-Chat, achieving competitive results on document-centric benchmarks.
However, these systems uniformly adopt vision encoders designed for natural images, applying the same patch tokenization and content-only attention to both photographs and structured documents.

\subsection{Vision Transformers and Token Efficiency}

Vision Transformers~\cite{dosovitskiy2021image} partition images into uniform patches and process them using self-attention.
While effective for natural images, this uniform treatment is suboptimal for documents where information density varies dramatically.
Token pruning approaches such as DynamicViT~\cite{rao2021dynamicvit} and token merging methods like ToMe~\cite{bolya2023token} reduce computational cost by removing or combining redundant tokens.
These methods modify the token set rather than the attention mechanism itself, and do not encode structural relationships between patches.
In contrast, our approach preserves the full token set while injecting structural bias directly into attention computation.

\subsection{Document Layout Modeling}

Document understanding has traditionally relied on layout detection and structured reasoning.
LayoutLM~\cite{xu2020layoutlm} and its successors~\cite{xu2021layoutlmv2,huang2022layoutlmv3} incorporate text position information through specialized embeddings.
Donut~\cite{kim2022donut} proposes OCR-free document understanding using an image-to-text framework.
DiT~\cite{li2022dit} applies self-supervised pretraining to document images.
Pix2Struct~\cite{lee2023pix2struct} parses screenshots using variable-resolution patching.
These approaches typically encode layout information through token embeddings or specialized pretraining objectives.
Our method differs by injecting structural relationships directly into the attention kernel, which modifies how tokens interact rather than how they are represented.

\subsection{Hallucination in Multimodal Models}

Hallucination in MLLMs manifests as generated content that is not grounded in the input image~\cite{rohrbach2018object,li2023evaluating}.
In document understanding, this particularly affects numeric values---the model may generate plausible but incorrect numbers when answering questions about charts and tables.
Recent work addresses hallucination through instruction tuning, reinforcement learning from human feedback, and contrastive decoding~\cite{zhou2024analyzing}.
Our approach complements these post-hoc methods by stabilizing visual grounding at the encoder level, reducing the likelihood of hallucination at its source.


%======================================================================
\section{Method}
\label{sec:method}
%======================================================================

\subsection{Preliminaries}

We build upon InternVL3.5-8B, which consists of InternViT-300M (a 24-layer Vision Transformer with 1024-dimensional embeddings and 16 attention heads) and InternLM2.5-7B-Chat as the language backbone.
The vision encoder processes $448 \times 448$ images using $14 \times 14$ patches, yielding a $32 \times 32$ grid of 1,024 spatial tokens plus one CLS token.
Each attention layer computes:
\begin{equation}
\text{Attn}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\label{eq:standard_attn}
\end{equation}
where $Q, K, V \in \mathbb{R}^{N \times d}$ are query, key, and value projections with head dimension $d = 64$.

\subsection{Structure-Factorized Attention}

We reformulate the attention score by adding a learnable structural bias:
\begin{equation}
S_{ij} = \frac{Q_i K_j^\top}{\sqrt{d}} + \phi(s_i, s_j),
\label{eq:sfa}
\end{equation}
where $s_i$ encodes the spatial position and block membership of patch $i$, and $\phi$ is a learnable structural bias function defined as:
\begin{equation}
\phi(s_i, s_j) = w_\text{row}^{(h)} \cdot \mathbb{1}[r_i = r_j] + w_\text{col}^{(h)} \cdot \mathbb{1}[c_i = c_j] - |w_\text{dist}^{(h)}| \cdot \hat{m}_{ij} + \bm{e}_{b_i}^{(h)\top} \bm{e}_{b_j}^{(h)},
\label{eq:phi}
\end{equation}
where $r_i, c_i$ denote the row and column indices of patch $i$ in the $32 \times 32$ grid, $\hat{m}_{ij}$ is the normalized Manhattan distance, and $b_i$ is the block assignment of patch $i$.
The superscript $(h)$ indicates that all parameters are per-head, enabling different attention heads to specialize in different structural aspects.

The structural bias consists of three components:

\textbf{Row and column alignment.} Same-row ($\mathbb{1}[r_i = r_j]$) and same-column ($\mathbb{1}[c_i = c_j]$) indicators encode horizontal and vertical alignment. These are precomputed binary matrices registered as buffers. The learnable weights $w_\text{row}, w_\text{col} \in \mathbb{R}^{H}$ control the strength of alignment bias per head.

\textbf{Distance decay.} The absolute-valued weight $|w_\text{dist}|$ applied to normalized Manhattan distance $\hat{m}_{ij} = (|r_i - r_j| + |c_i - c_j|) / (H_p + W_p)$ introduces a monotonic penalty for spatial separation, encouraging local attention patterns.

\textbf{Block-level similarity.} Each patch is assigned to one of $B=16$ blocks via density-guided assignment (Section~\ref{sec:density}). Block embeddings $\bm{e}_b \in \mathbb{R}^{H}$ are learned, and same-block similarity $\bm{e}_{b_i}^\top \bm{e}_{b_j}$ encourages attention within semantically coherent regions.

The structural bias is applied only to spatial tokens, preserving the CLS token's content-only attention:
\begin{equation}
S[:, :, 1{:}, 1{:}] \mathrel{+}= \phi(\cdot, \cdot).
\end{equation}

\textbf{Parameter count.} Each layer adds $3H + BH = 3 \times 16 + 16 \times 16 = 304$ parameters. Across 24 layers, SFA introduces $\mathbf{7{,}296}$ parameters total---$0.002\%$ of the 4.7B-parameter base model. These are initialized with $\mathcal{N}(0, 0.02^2)$ to minimally perturb pretrained attention patterns.

\subsection{Text Density Estimation}
\label{sec:density}

To provide block assignments for SFA, we train a lightweight \textbf{Text Density Estimator}: a 6-layer CNN (186K parameters) that maps $448 \times 448$ images to $28 \times 28$ density maps $D \in [0,1]^{28 \times 28}$.

\textbf{Self-supervised training.} We generate pseudo-labels using CRAFT~\cite{baek2019craft} text detection on high-resolution images: character-level heatmaps are downsampled to $28 \times 28$ and normalized to $[0,1]$. The estimator is trained with MSE loss on ChartQA training images. Once trained, it is frozen and used as a fixed preprocessing module.

\textbf{Density-guided block assignment.} Given a density map $D$, we resize it to the $32 \times 32$ patch grid via bilinear interpolation and quantize density values into $B=16$ discrete blocks:
\begin{equation}
b_i = \min\!\big(\lfloor D_i \cdot B \rfloor,\, B-1\big), \quad i \in \{1, \ldots, N\}.
\label{eq:block_assign}
\end{equation}
This simple quantization provides a content-adaptive grouping: patches in text-dense regions share higher block IDs, while background patches cluster in lower blocks. The block IDs are passed to SFA's block embedding layer, enabling the structural bias to encode semantic similarity beyond spatial proximity.

\subsection{Structural Consistency Regularization}
\label{sec:scr}

To explicitly encourage sharper attention patterns on text-dense regions, we introduce an auxiliary \textbf{attention entropy regularization} loss.

Given density mask $M \in \{0,1\}^{N}$ (patches where $D_i > \tau$ with threshold $\tau = 0.3$), we compute the entropy of attention distributions for text-dense query patches across selected layers:
\begin{equation}
\mathcal{L}_\text{entropy} = \frac{1}{|\mathcal{L}|} \sum_{l \in \mathcal{L}} \frac{\sum_{h,i} M_i \cdot H(\bm{\alpha}_{l,h,i})}{\sum_{h,i} M_i},
\label{eq:entropy_loss}
\end{equation}
where $H(\bm{\alpha}) = -\sum_j \alpha_j \log \alpha_j$ is the Shannon entropy of attention weights, and $\mathcal{L} = \{18, 19, 20, 21, 22, 23\}$ denotes the last 6 layers.
Minimizing entropy on text-dense patches encourages focused attention on informative regions without affecting sparse background patches.

The overall training objective is:
\begin{equation}
\mathcal{L} = \mathcal{L}_\text{task} + \alpha \cdot \mathcal{L}_\text{entropy},
\label{eq:total_loss}
\end{equation}
where $\mathcal{L}_\text{task}$ is the standard autoregressive language modeling loss and $\alpha = 0.1$.

\subsection{Training Strategy}
\label{sec:training}

A critical design choice is the \textbf{frozen backbone strategy}: the entire vision encoder, vision-language projector, and language model are frozen; only the 7,296 SFA structural bias parameters are trained. This has two key advantages:

\textbf{Avoiding catastrophic forgetting.} Fine-tuning the full vision encoder (337M parameters) leads to catastrophic forgetting: in our experiments, full fine-tuning reduces ChartQA accuracy from 62.0\% to 50.9\%---a 17.9\% drop---as the pretrained visual representations are overwritten.

\textbf{Isolating structural contribution.} By training only SFA parameters, any performance change is directly attributable to the structural bias, providing a clean ablation of the structural inductive bias hypothesis.

Training proceeds for 3 epochs on ChartQA training data (28K samples) with AdamW optimizer ($\text{lr} = 10^{-3}$, cosine decay), batch size 32 (4 per GPU $\times$ 8 gradient accumulation steps), and bfloat16 mixed precision. The language model uses 4-bit NF4 quantization~\cite{dettmers2024qlora} to fit within a single 40GB A100 GPU.


%======================================================================
\section{Experiments}
\label{sec:experiments}
%======================================================================

\subsection{Experimental Setup}

\textbf{Base model.} InternVL3.5-8B~\cite{chen2024far}, consisting of InternViT-300M (24 layers, 1024-dim, 16 heads) and InternLM2.5-7B-Chat with a 2-layer MLP projector.

\textbf{Benchmarks.} Primary evaluation on ChartQA~\cite{masry2022chartqa} test set (2,500 samples) using Relaxed Accuracy (correct if prediction is within 5\% of the ground truth for numeric answers, or exact match for categorical answers). Cross-benchmark generalization is evaluated on DocVQA~\cite{mathew2021docvqa} (ANLS metric), InfographicVQA (ANLS), DVQA (Exact Match), and FigureQA (Exact Match), each with 500 randomly sampled test instances. For mixed-benchmark training, we use a 45,572-sample mixture: ChartQA (18K), DocVQA (10K), DVQA (10K subsampled), FigureQA (5K subsampled), and InfographicVQA (2K).

\textbf{Hallucination evaluation.} Following prior work on hallucination analysis, we define \textit{numeric hallucination} as cases where the model produces a numeric answer that differs from the ground truth by more than 5\%, while the ground truth is also numeric. We report hallucination rate on both the full test set and a fixed 200-sample subset (seed=42) for detailed breakdown analysis.

\textbf{Hardware.} 2$\times$ NVIDIA A100-PCIE-40GB. Single-GPU mode with 4-bit quantization for the language model.

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Ablation study on ChartQA test set (2,500 samples). All SFA variants use the frozen backbone strategy with only structural bias parameters trained. ``Full ft'' fine-tunes the entire vision encoder (337M parameters).}
\label{tab:main}
\begin{tabular}{@{}lrcc@{}}
\toprule
\textbf{Configuration} & \textbf{Trainable Params} & \textbf{ChartQA Acc} & \textbf{Halluc Rate} \\
\midrule
Baseline (InternVL3.5-8B) & 0 & 62.0\% & 25.5\% \\
+ SFA (full encoder ft) & 337M & 50.9\% & 23.0\% \\
\midrule
+ SFA (frozen backbone) & 7,296 & 62.4\% & 20.2\% \\
+ SFA + ADAT & 7,296 & 62.8\% & 20.0\% \\
\textbf{+ SFA + ADAT + SCR} & \textbf{7,296} & \textbf{62.9\%} & \textbf{20.0\%} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:main} presents the main results. Our full system (SFA + ADAT + SCR) achieves the best performance with 62.9\% accuracy (+0.9\%p over baseline) and 20.0\% hallucination rate ($-5.5$\%p reduction), using only 7,296 trainable parameters.

\textbf{Catastrophic forgetting.} Fine-tuning the full vision encoder with SFA (337M trainable parameters) causes accuracy to drop to 50.9\%, a 17.9\% decrease. The pretrained visual representations are overwritten, destroying general recognition capabilities. This motivates the frozen backbone approach.

\textbf{Incremental contribution.} Each component provides additive improvement: SFA alone reduces hallucination from 25.5\% to 20.2\% ($-5.3$\%p); density-guided block assignment (ADAT) further improves accuracy (+0.4\%p) and hallucination ($-0.2$\%p); entropy regularization (SCR) provides a final refinement (+0.04\%p accuracy).

\subsection{Hallucination Analysis}

\begin{table}[t]
\centering
\caption{Detailed hallucination breakdown on 200-sample subset (seed=42). Each prediction is classified as correct, numeric hallucination, or other error.}
\label{tab:halluc}
\begin{tabular}{@{}lccccc@{}}
\toprule
& \textbf{Baseline} & \textbf{Full ft} & \textbf{SFA} & \textbf{+ADAT} & \textbf{+SCR} \\
\midrule
Correct & 130 & 105 & 130 & 130 & \textbf{131} \\
Numeric hallucination & 51 & 46 & 39 & 39 & \textbf{38} \\
Other errors & 19 & 49 & 31 & 31 & 31 \\
\midrule
Hallucination rate & 25.5\% & 23.0\% & 19.5\% & 19.5\% & \textbf{19.0\%} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:halluc} provides a detailed breakdown on a fixed 200-sample subset.
SFA with frozen backbone reduces numeric hallucination from 51 to 38 cases ($-25.5\%$), with the full system (SFA+ADAT+SCR) achieving 131 correct answers (+1 over baseline) and 38 hallucination cases.

Notably, the full encoder fine-tuning approach reduces hallucination (51$\to$46) but dramatically increases ``other errors'' (19$\to$49), indicating that the model loses its ability to correctly answer questions that do not involve numeric grounding.
The frozen backbone approach avoids this trade-off entirely.

\subsection{Attention Entropy Analysis}

We analyze attention entropy across the 24 vision encoder layers, comparing baseline and SFA-trained models.
We define \textit{text-dense patches} as those where the density estimator output exceeds 0.3, and \textit{sparse patches} as the remainder.

In the baseline model, text-dense and sparse patches exhibit nearly identical mean entropy (4.33 vs.\ 4.44 nats, ratio 0.98$\times$), confirming that the pretrained encoder does not differentiate between information-rich and empty regions.

After SFA training, the overall attention distribution is reshaped by the structural bias, with row and column alignment encouraging more structured attention patterns.
The SCR entropy regularization further reduces text-dense entropy from 4.082 to 4.075 nats across the last 6 layers, though the effect is modest due to the limited capacity of the 7,296 trainable parameters.

\subsection{Cross-Benchmark Generalization}

\begin{table}[t]
\centering
\caption{Cross-benchmark evaluation (500 samples each). ``ChartQA-SFA'' trains SFA on ChartQA only; ``Mixed-SFA'' trains on a 45K-sample mixture of all five benchmarks. Mixed training closes the cross-benchmark gap while maintaining ChartQA gains.}
\label{tab:cross}
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Benchmark} & \textbf{Metric} & \textbf{Baseline} & \textbf{ChartQA-SFA} & \textbf{Mixed-SFA} & \textbf{$\Delta_{\text{mixed}}$} \\
\midrule
ChartQA (2,500) & Relaxed Acc & 62.0\% & \textbf{62.9\%} & 62.6\% & +0.6 \\
DocVQA & ANLS & 53.6\% & 52.9\% & \textbf{53.5\%} & $-0.1$ \\
InfographicVQA & ANLS & 38.7\% & 38.5\% & \textbf{38.7\%} & $\pm$0.0 \\
DVQA & Exact Match & 41.0\% & 40.8\% & \textbf{41.0\%} & $\pm$0.0 \\
FigureQA & Exact Match & \textbf{95.6\%} & \textbf{95.6\%} & 95.4\% & $-0.2$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:cross} evaluates generalization across document-centric benchmarks.
When trained on ChartQA alone, SFA achieves strong in-domain improvement (+0.9\%p accuracy, $-5.5$\%p hallucination) but shows modest degradation on unseen benchmarks (up to $-0.7$\%p on DocVQA).
Training SFA on a diverse mixture of five benchmarks (45,572 samples: ChartQA 18K, DocVQA 10K, DVQA 10K, FigureQA 5K, InfographicVQA 2K) closes this gap: DocVQA improves from $-0.7$\%p to $-0.1$\%p, InfographicVQA from $-0.2$\%p to $\pm 0.0$\%p, and DVQA from $-0.2$\%p to $\pm 0.0$\%p, while ChartQA accuracy remains above baseline at $+0.6$\%p.
This demonstrates that the structural bias generalizes across document types when trained on diverse data, and the frozen backbone strategy prevents catastrophic forgetting regardless of training mixture.

\subsection{Parameter Efficiency}

\begin{table}[t]
\centering
\caption{Parameter efficiency comparison. SFA achieves meaningful hallucination reduction with 46,000$\times$ fewer trainable parameters than full fine-tuning.}
\label{tab:params}
\begin{tabular}{@{}lrcl@{}}
\toprule
\textbf{Strategy} & \textbf{Params} & \textbf{$\Delta$Halluc} & \textbf{Params/\%p Reduction} \\
\midrule
Full encoder ft & 337M & $-2.5$\%p & 135M per \%p \\
LoRA (typical) & $\sim$4M & --- & --- \\
\textbf{SFA (frozen)} & \textbf{7,296} & $\bm{-5.5}$\textbf{\%p} & \textbf{1,327 per \%p} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:params} highlights the extreme parameter efficiency of SFA.
While full fine-tuning requires 135M parameters per percentage point of hallucination reduction, SFA achieves a larger reduction with only 1,327 parameters per percentage point---a 100,000$\times$ improvement in parameter efficiency.
This suggests that the bottleneck for document grounding is not model capacity but rather the absence of appropriate structural inductive bias.


%======================================================================
\section{Discussion}
%======================================================================

\textbf{Why does frozen backbone work?}
SFA operates as an \textit{additive structural prior} on top of the pretrained content-based attention.
The structural bias $\phi(s_i, s_j)$ shifts attention scores towards structurally related patches (same row, same column, same block) without overwriting the learned query-key interactions.
Since the bias is initialized near zero ($\sigma = 0.02$), the initial model behavior is identical to the baseline, and training gradually introduces structural preferences.

\textbf{Why do 7,296 parameters suffice?}
Structural relationships in documents are inherently low-dimensional: row alignment, column alignment, and spatial proximity can be captured by just 3 scalar weights per head. The block embedding adds adaptive grouping with 16 entries per head.
This compact representation is sufficient because the structural prior does not need to model \textit{what} is in each patch---that is handled by the pretrained content attention---but only \textit{where} patches are relative to each other.

\textbf{Limitations.}
The improvement magnitude is modest ($+0.6$\%p accuracy, $-5.5$\%p hallucination on ChartQA with mixed training), and the entropy regularization effect is small, likely because the 7,296 SFA parameters have limited capacity to reshape attention distributions dominated by the frozen 337M-parameter backbone.
Cross-architecture validation on other vision encoders (SigLIP, CLIP) remains for future work.

\textbf{Future directions.}
Promising extensions include: (1) applying SFA to SigLIP-based models (Qwen2.5-VL) and CLIP-based models (LLaVA) to validate architecture-agnostic claims; (2) exploring stronger regularization objectives such as numeric grounding loss with bounding-box supervision; and (3) unfreezing SFA parameters during full instruction tuning as a complementary fine-tuning strategy.


%======================================================================
\section{Conclusion}
%======================================================================

We presented Structure-Factorized Attention (SFA), a minimal modification to the vision encoder that injects document-aware structural bias into the self-attention mechanism.
By decomposing attention into content and structural components, SFA enables layout-aware token interactions with only 7,296 additional parameters (0.002\% of the base model).
Combined with density-guided block assignment and attention entropy regularization, our approach reduces numeric hallucination by 5.5 percentage points on ChartQA while maintaining or improving accuracy.
Training SFA on a diverse mixture of five document benchmarks further demonstrates that the structural bias generalizes across document types, closing the cross-benchmark gap to within $\pm 0.2$\%p of the baseline while preserving in-domain gains.

The frozen backbone strategy demonstrates that structural inductive bias, rather than increased model capacity, is the key missing element for reliable document grounding.
Our results suggest that even extremely lightweight architectural modifications can meaningfully improve the faithfulness of multimodal models on structured visual inputs, opening a parameter-efficient path toward more reliable document understanding.


%======================================================================
\bibliographystyle{splncs}
\bibliography{egbib}
\end{document}
