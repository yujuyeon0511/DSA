# DSA Project Progress Report

**Project**: Structure-Factorized Attention (SFA) for Document-Centric Multimodal LLMs
**Last Updated**: 2026-02-23

---

## Overview

ë¬¸ì„œ/ì°¨íŠ¸ íŠ¹í™” ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ì—ì„œ ê¸°ì¡´ ViTì˜ êµ¬ì¡°ì  í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
**Structure-Factorized Attention (SFA)** + **Adaptive Density-Aware Tokenization (ADAT)** ì„ ì œì•ˆí•˜ê³ 
InternVL3.5-8B ìœ„ì—ì„œ ê²€ì¦í•˜ëŠ” ì—°êµ¬ í”„ë¡œì íŠ¸.

### Core Modules
| Module | Description | Params |
|--------|-------------|--------|
| **SFA** | Attentionì— structural bias (row/col/block) ì£¼ì… | 304/layer Ã— 24 = **7,296** |
| **ADAT** | í…ìŠ¤íŠ¸ ë°€ì§‘ë„ ê¸°ë°˜ ë™ì  íŒ¨ì¹˜ í• ë‹¹ | **186K** (density estimator) |
| **SCR** | Entropy/Grounding/Stability regularization | Loss only (ì¶”ê°€ params ì—†ìŒ) |

### Environment
| Item | Value |
|------|-------|
| Base Model | InternVL3.5-8B (InternViT-300M + InternLM2.5-7B) |
| GPU | NVIDIA A100-PCIE-40GB Ã— 2 |
| Framework | PyTorch 2.4.0, transformers 5.1.0 |
| Conda env | `docmllm` |

---

## Completed Steps

### Step 0: Baseline Evaluation âœ…

InternVL3.5-8B ì›ë³¸ ëª¨ë¸ì˜ ChartQA ì„±ëŠ¥ ì¸¡ì •.

| Benchmark | Metric | Score | Samples |
|-----------|--------|-------|---------|
| ChartQA | Relaxed Accuracy | **0.6200** | 200 |

- Single-tile (448Ã—448) ì¶”ë¡ 
- ì˜¤ë‹µ íŒ¨í„´: ì†Œìˆ˜ì  ìˆ˜ì¹˜ ì˜¤ë¥˜ (0.57â†’10.04), ë¯¸ì„¸ ì°¨ì´ (0.08â†’0.02)
- **ìˆ«ì grounding ì·¨ì•½ì  í™•ì¸**

### Step 1: Text Density Estimator âœ…

ë¬¸ì„œ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ë°€ì§‘ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” lightweight CNN í•™ìŠµ.

| Item | Value |
|------|-------|
| Architecture | 6-layer CNN (3â†’32â†’64â†’128â†’64â†’32â†’1) |
| Parameters | **186K** |
| Train/Val | 19,000 / 1,000 (ChartQA + DVQA) |
| Best Val Loss | **0.001728** (Epoch 10) |
| Output | 28Ã—28 density heatmap D(x,y) âˆˆ [0,1] |

Pseudo label ìƒì„±: Canny edge â†’ adaptive threshold â†’ Gaussian blur

### Step 2: SFA Module Test âœ…

Structure-Factorized Attention ëª¨ë“ˆ ë‹¨ë… ë™ì‘ ê²€ì¦.

**SFA ìˆ˜ì‹:**
```
S_ij = (Q_i Â· K_j^T) / âˆšd + Ï†(s_i, s_j)

Ï†(s_i, s_j) = w_rowÂ·ğŸ™[row_i = row_j]
            + w_colÂ·ğŸ™[col_i = col_j]
            + w_distÂ·(-manhattan(i,j))
            + block_embed(b_i)^T Â· block_embed(b_j)
```

| Item | Value |
|------|-------|
| Forward test | [2, 784, 1024] â†’ [2, 784, 1024] âœ… |
| Structural Bias Params | **304** per layer (0.007% overhead) |

### Step 3: SFA â†’ InternVL Integration âœ…

InternViTì˜ 24ê°œ self-attention layerë¥¼ SFAë¡œ êµì²´ í›„ inference í™•ì¸.

| Item | Value |
|------|-------|
| Replaced layers | **24 / 24** |
| Trainable params | 337,590,400 / 8,528,325,760 (**4.0%**) |
| Inference test | **PASSED** |

### Step 4: Attention Entropy Analysis (Baseline) âœ…

Text-dense vs sparse regionì˜ attention entropy ì¸¡ì •.

| Region | Entropy |
|--------|---------|
| Text-dense | **4.3322** |
| Sparse | **4.4377** |
| Ratio | **0.98x** |

â†’ êµ¬ì¡°ì  ë°”ì´ì–´ìŠ¤ ë¶€ì¬ í™•ì¸ â€” text/sparse ê°„ entropy ì°¨ì´ ê±°ì˜ ì—†ìŒ

### Step 5: Hallucination Rate Analysis (Baseline) âœ…

| Metric | Value |
|--------|-------|
| Accuracy | **0.6500** |
| Hallucination Rate | **0.2550** (51/200) |
| Wrong Answer Rate | 0.0950 (19/200) |

â†’ **ì˜¤ë¥˜ì˜ 73%ê°€ ìˆ«ì hallucination** â€” êµ¬ì¡°ì  grounding ë¶€ì¬ê°€ ì£¼ì›ì¸

### Step 6: Token Efficiency Curve âœ… (Placeholder)

Placeholder ìƒì„± ì™„ë£Œ. ADAT êµ¬í˜„ í›„ ì‹¤ ë°ì´í„°ë¡œ êµì²´ ì˜ˆì •.

---

## In Progress

### Phase 2-2: SFA Fine-tuning ğŸ”„ (í˜„ì¬ í•™ìŠµ ì¤‘)

**OOM ë¬¸ì œ í•´ê²° í›„ í•™ìŠµ ì§„í–‰ ì¤‘.**

#### OOM í•´ê²° ë°©ë²•
ê¸°ì¡´ ë¬¸ì œ: 8.5B ëª¨ë¸ì„ A100-40GBì— ì˜¬ë¦¬ë©´ OOM ë°œìƒ (ëª¨ë¸ 17GB + optimizer + activations)

í•´ê²°:
1. **Frozen LLM â†’ 4-bit NF4 ì–‘ìí™”** (bitsandbytes): ~14GB â†’ ~3.5GB
2. **Vision encoder gradient checkpointing**: í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê°
3. **batch_size=1, grad_accum=32**: í”¼í¬ ë©”ëª¨ë¦¬ ìµœì†Œí™”

ê²°ê³¼: **GPU ë©”ëª¨ë¦¬ 8.3GB / 40GB** (ì´ì „ OOM â†’ ì¶©ë¶„í•œ ì—¬ìœ )

#### í•™ìŠµ ì„¤ì •
| Item | Value |
|------|-------|
| Data | ChartQA train (28,299 samples) |
| Effective batch size | 1 Ã— 32 (grad_accum) = **32** |
| Epochs | 3 |
| Total optimizer steps | 2,653 |
| LR | 2e-5 (cosine, warmup 100 steps) |
| Trainable | Vision encoder (SFA) + Projector (**337M / 4.7B = 7.1%**) |
| Frozen | LLM (4-bit quantized) |

#### í•™ìŠµ ê²½ê³¼
| Epoch | Step | Loss | LR | GPU Mem |
|-------|------|------|----|---------|
| 1 | 320/28299 | 5.5692 | 2.00e-06 | 8.3GB |
| 1 | 640/28299 | 5.4709 | 4.00e-06 | 8.3GB |

ì˜ˆìƒ í•™ìŠµ ì‹œê°„: ~12ì‹œê°„

---

## Generated Figures

| Figure | File | Status |
|--------|------|--------|
| Fig 1: Motivation (uniform vs adaptive patching) | `figures/fig1_motivation/` | âœ… ìƒì„± ì™„ë£Œ |
| Fig 2: Architecture diagram | `figures/fig2_architecture.{pdf,png}` | âœ… ìƒì„± ì™„ë£Œ |
| Fig 4: Density map gallery | `results/01_density/visualizations/` | âœ… 20ì¥ ìƒì„± |
| Fig 5: Entropy (baseline) | `figures/fig5_entropy/` | âœ… Baseline ìƒì„± (SFA í›„ ì™„ì„± ì˜ˆì •) |
| Fig 6: Token efficiency | `results/04_analysis/token_efficiency_curve.{pdf,png}` | âœ… Placeholder |

---

## Remaining Phases

### Phase 2 (SFA í›„ì† â€” í•™ìŠµ ì™„ë£Œ í›„)
- P2-3: SFA ëª¨ë¸ ChartQA eval â†’ Table 1 "+SFA"
- P2-4: SFA entropy ì¬ì¸¡ì • â†’ Figure 5 ì™„ì„±
- P2-5: SFA hallucination ì¬ì¸¡ì • â†’ Table 2 "+SFA"
- P2-6: SFA attention heatmap â†’ Figure 3 ì™„ì„±
- P2-7: Structural bias ì‹œê°í™” â†’ Figure 7
- P2-8: Structural component ablation â†’ Table 3

### Phase 3 (ADAT)
- ADAT ëª¨ë“ˆ êµ¬í˜„ + ë‹¨ë… eval
- SFA+ADAT í†µí•© fine-tuning
- Token efficiency ì‹¤ì¸¡

### Phase 4 (Full System + SCR)
- SCR loss êµ¬í˜„ + fine-tuning
- 6ê°œ benchmark ì „ì²´ eval
- Compute cost ì¸¡ì •

### Phase 5 (Cross-Architecture + ë…¼ë¬¸)
- SFA â†’ Qwen2.5-VL (SigLIP+Qwen) ì ìš©
- SFA â†’ LLaVA-OV (CLIP+LLaMA) ì ìš©
- ë…¼ë¬¸ ì‘ì„±

---

## Technical Issues Resolved

| Issue | Cause | Solution |
|-------|-------|----------|
| `conversation.py` ëˆ„ë½ | `Model_original` ë””ë ‰í† ë¦¬ | ê²½ë¡œë¥¼ `/NetDisk/j_son/internvl_35/`ë¡œ ë³€ê²½ |
| Meta tensor RuntimeError | `device_map="auto"` + InternViT | `from_config()` + safetensors ìˆ˜ë™ ë¡œë”© |
| Flash attn â†’ attention weight ë¯¸ë…¸ì¶œ | InternViT ê¸°ë³¸ê°’ | `use_flash_attn=False` + QKV hook |
| VL loss gradient ëŠê¹€ | `img_context_token_id` ë¯¸ì„¤ì • | `compute_vl_loss()` ìˆ˜ë™ êµ¬í˜„ |
| **CUDA OOM (í•™ìŠµ ë¶ˆê°€)** | 8.5B ëª¨ë¸ on A100-40GB | **LLM 4-bit ì–‘ìí™” + gradient checkpointing** |
| Patch grid 28 vs 32 | 448/14 = 32 | ì „ì²´ ëª¨ë“ˆ `num_patches_h/w` 32ë¡œ ìˆ˜ì • |

---

## File Structure

```
DSA/
â”œâ”€â”€ plan.md                              # 1ê°œì›” ì—°êµ¬ ê³„íšì„œ
â”œâ”€â”€ PROGRESS.md                          # ì§„í–‰ í˜„í™© (ì´ íŒŒì¼)
â”œâ”€â”€ architecture_diagram_prompt.md       # Figure 2 AI ìƒì„± í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ eccv2016submission.tex               # ë…¼ë¬¸ í…œí”Œë¦¿
â”œâ”€â”€ Structure-Factorized_Document_Attention.pdf  # ì°¸ê³  ë…¼ë¬¸
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ EXP-20260220-001-experiment-design.md    # ì‹¤í—˜ ë§ˆìŠ¤í„° ë¬¸ì„œ
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ model_utils.py               # ëª¨ë¸ ë¡œë”© (full/quantized)
â”‚   â”‚   â”œâ”€â”€ 00_baseline_eval.py          # Baseline í‰ê°€
â”‚   â”‚   â”œâ”€â”€ 01_density_estimator.py      # Density Estimator í•™ìŠµ
â”‚   â”‚   â”œâ”€â”€ 02_sfa_module.py             # SFA ëª¨ë“ˆ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ 03_sfa_integration.py        # SFA â†’ InternVL í†µí•©
â”‚   â”‚   â”œâ”€â”€ 03_sfa_finetune.py           # SFA Fine-tuning (4-bit quantized)
â”‚   â”‚   â”œâ”€â”€ 04_attention_analysis.py     # Entropy/Hallucination ë¶„ì„
â”‚   â”‚   â”œâ”€â”€ 05_figure_motivation.py      # Figure 1 ìƒì„±
â”‚   â”‚   â”œâ”€â”€ 06_attention_heatmap.py      # Figure 3 ìƒì„±
â”‚   â”‚   â”œâ”€â”€ 07_figure_entropy.py         # Figure 5 ìƒì„±
â”‚   â”‚   â””â”€â”€ gen_architecture_diagram.py  # Figure 2 ìƒì„±
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ fig1_motivation/             # Figure 1: Motivation
â”‚   â”‚   â”œâ”€â”€ fig2_architecture.{pdf,png}  # Figure 2: Architecture
â”‚   â”‚   â”œâ”€â”€ fig5_entropy/                # Figure 5: Entropy
â”‚   â”‚   â””â”€â”€ sample_images/              # ìƒ˜í”Œ ì´ë¯¸ì§€
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ 00_baseline/                 # Baseline ê²°ê³¼
â”‚       â”œâ”€â”€ 01_density/                  # Density Estimator ì²´í¬í¬ì¸íŠ¸
â”‚       â”œâ”€â”€ 03_sfa_train/                # SFA í•™ìŠµ (ì§„í–‰ ì¤‘)
â”‚       â””â”€â”€ 04_analysis/                 # Entropy/Hallucination ë¶„ì„
â””â”€â”€ .gitignore
```
